# Author: Claudio <3261958605@qq.com>
# Created: 2017-06-02 11:40:31
# Commentary:
#+TITLE: 并发执行

[[file:~/Desktop/Python/resources/site/docs.python.org/3.5/library/concurrency.html][本地页面]]

本章介绍的模块可实现并发执行代码。是否选用并发执行取决于程序需求，榨干
CPU性能还是IO性能，协作多任务还是优先多任务。

* threading - 以线程为基础并发
  参考：

  1. [[http://yoyzhou.github.io/blog/2013/02/28/python-threads-synchronization-locks/][@yoyzhou（可能Event小节有bug，去掉sleep(...)可发现）]]
  2. [[http://www.cnblogs.com/huxi/archive/2010/06/26/1765808.html][@AstralWind]]
  3. [[http://www.ywlib.com/archives/19.html][@一闻自习室]]

  threading模块主要用来同时运行I/O操作频繁的任务，如需利用多核CPU，可
  参考multiprocessing模块。

  本模块建立在底层_thread模块上，提供操作线程的高阶接口。还可参见queue
  模块。

  如果缺少_thread模块，则不能使用threading模块，此时可使用
  dummy_threading模块作为替代品。

** 模块函数
*** threading. *active_count* ()
    当前存活的Thread对象数量，等于threading.enumerate()调用结果的长度。

    #+BEGIN_SRC python :session
      import threading
      print('当前Thread对象数量：', threading.active_count())
    #+END_SRC

    #+RESULTS:
    #+BEGIN_SRC org
    - >>> 当前Thread对象数量： 1
    #+END_SRC

*** threading. *current_thread* ()
    返回当前Thread对象，与控制调用环境的线程相关。如果调用函数在不为通过
    threading模块创建，则返回功能优先的“虚假”线程对象。？？？
*** threading. *get_ident* ()
    返回当前线程的“身份”，为非0整数。此数值并没有直接的实际含义，主要
    用来作为一个“魔法cookie”。比如用来索引线程相关的字典。当线程退出，
    另一个线程创建时，线程“身份”可能被垃圾回收，并被重新使用。

    #+BEGIN_SRC python :session
      import threading
      print(threading.current_thread())
      print(threading.get_ident())
    #+END_SRC

    #+RESULTS:
    #+BEGIN_SRC org
    - <_MainThread(MainThread, started 140373658789632)>
    - 140373658789632
    #+END_SRC

*** threading. *enumerate* ()
    返回当前所有存活线程注册的链表。

    包含：后台线程（daemonic threads）、current_thread()创建的“虚假”线
    程（dummy thread）、以及主线程。

    不包括：已经被终止的线程，还没开始执行的线程。

*** threading. *main_thread* ()
    返回主线程Thread对象。普通情况下，当Python解释器开启时就启动主线程。

*** threading. *settrace* (func) ？？？
    为所有threading模块创建的线程添加跟踪函数（“trace function”）。在
    执行run()方法前，FUNC参数会传递给sys.settrace()函数。

*** threading. *setprofile* (func) ？？？
    为所有threading模块创建的线程添加profile函数。在执行run()方法前，
    FUNC参数会传递给sys.setprofile()函数。

*** threading. *stack_size* ([size]) ？？？
    返回创建新线程时，线程使用的stack大小。
    
    size：可选参数，指定以后创建线程时分配的stack大小。默认为0，即使用
    系统默认配置。如果设置，则需为>=32,768（32KiB）。如果不支持设置，
    则抛出RuntimeError错误；如果数值无效，则抛出ValueError错误，不做修
    改。目前支持的最小stack为32KiB。

    设置时，对系统有依赖。（待续）

** 模块常量
*** threading. *TIMEOUT_MAX*
    本模块中函数/构造器TIMEOUT参数允许的最大值，如Lock.acquire()、
    Rlock.acquire()和Condition.wait()等。如果超过此值，则抛出
    OverflowError错误。

    #+BEGIN_SRC python :session
      import threading
      threading.TIMEOUT_MAX
    #+END_SRC

    #+RESULTS:
    #+BEGIN_SRC org
    - 9223372036.0
    #+END_SRC

** 线程本地变量 ？？？
   线程本地变量，其值只在线程中有效。使用threading.local（或子类），可
   创建线程本地变量。

   #+BEGIN_EXAMPLE python
     mydata = threading.local()
     mydata.x = 1
   #+END_EXAMPLE

   实例的值在不同线程中相互独立。

*** class threading. *local*
    表示线程本地变量数据的类。

    具体用法可参考_threading_local模块的文档。

** Thread对象
   Thread类表示独立允许的线程活动。有两种办法创建线程：

   1. 在构造器中传递可调用对象。
   2. 使用子类，重置run()方法。

      *声明Thread子类时，只能重置__init__()和run()方法。*

   当创建Thread对象后，只有调用其start()方法才能激活线程。激活后在独立
   的线程中运行run()方法。

   当线程启动后，则表示线程“存活”。当run()方法执行结束——正常结束或抛出
   错误没被处理，则线程“终止”。可使用is_alive()方法判断线程使用“存活”。

   线程可调用join()方法，调用后，直到调用join()方法的线程的join()方法
   执行结束后，才会执行当前线程中的代码。

   Thread对象具有name属性，可在构造器中指定，此属性可写。

   当线程声明为“后台线程”时，即声明构造器的daemon参数为True，如果主线
   程退出时，“后台线程”还没执行完，也会随之退出。可通过修改Thread对象
   的daemon属性，或在构造器中声明。是否为“后台线程”，默认继承于创建它
   的线程。 *声明daemon=True的线程，不能调用join()方法。*

   #+BEGIN_SRC python :session
     import threading as td
     import time

     def job():
         time.sleep(0.1)
         print('执行子线程函数。')

     def multi_thread():
         threads = []
         for i in range(10):
             t = td.Thread(target=job, daemon=True)
             t.start()

     multi_thread()
     print('主线程执行结束')
   #+END_SRC

   #+RESULTS:
   #+BEGIN_SRC org
   - >>> >>> ... ... ... >>> ... ... ... ... ... >>> >>> 主线程执行结束
   #+END_SRC

   #+BEGIN_QUOTE
   *注意* ： “后台线程”退出可能非常“唐突”，如不能合理释放资源（如已打
   开文件和数据库事务）。如需优雅退出“后台线程”，可使其“非后台化”，并
   使用合适的信号机制，如后面介绍的threading.Event对象。
   #+END_QUOTE

   总是有一个“主线程”，即控制Python程序的初始线程，从不为“后台线程”。

   有可能会自动创建“虚拟线程对象（dummy thread objects）”。他们为“外部
   线程（alien threads）”，在threading模块外创建，如直接从C代码创建。
   “虚拟线程”功能有限，且总是“存活”，并且为“后台线程”，不能调用join()
   方法，不能被删除。

*** class threading. *Thread* (group=None,target=None,name=None,args=(),kwargs={},*,daemon=None)

    *使用此构造器时，只能使用关键字参数。* 各参数的含义为：

    group：需为None，为以后支持ThreadGroup实现扩展预留的参数。

    target：可调用对象，被run()方法激活。默认为None，即不进行任何调用。

    name：线程名称。构造时默认创建"Thread-N"，其中N为十进制数。

    args：target指定可调用对象的参数，默认为()，如果为单元素元组，须为
    (arg0,)形式。

    kwargs：字典对象，target指定可调用对象的关键字参数，默认为{}。

    daemon：明确声明新建线程是否为“后台线程”。如果为None（默认值），
    则根据创建线程的线程是否为“后台线程”而定。

    *如果子类重载此构造器，则需保证在__init__()声明中，做任何事前声明
    Thread.__init__()。*

**** start()
     激活线程。

     只能调用一次。目的时将Thread对象的run()方法放置在新建的线程中执行。

     如果调用多次，则抛出RuntimeError错误。

**** run()
     表示线程活动的方法。

     可在子类中重置此方法；标准的run()方法会调用构造器中target参数指定
     的可调用对象，并让其调用args和kwargs指定的参数。

**** join(timeout=None)
     等待直到线程执行结束。使所在线程中的代码，直到该线程的join()方法
     调用终止（正常结束、抛出错误不被处理、超过timeout指定时间）后才执
     行。

     timeout：如果不为None，须为指定秒数的浮点数。由于join()方法总是返
     回None，所以如果使用timeout参数，需使用is_alive()方法判断子线程是
     否依然“存活”，再决定是否重新调用join()方法。

     当timeout参数为None或缺省时，所有代码会被阻塞至子线程执行结束后才
     执行。

     一个线程对象可调用多次join()方法。

     如果对可能造成“锁死”的线程对象调用join()方法，会抛出RuntimeError
     错误。如果在调用start()方法前调用join()方法，也会抛出RuntimeError
     错误。

     #+BEGIN_SRC python :session
       import threading as td
       import time

       def job():
           time.sleep(2)
           print('子线程执行结束。')

       def multi_thread():
           t1 = td.Thread(target=job)
           t1.start()
           t1.join(1)
           print('子线程是否存活：', t1.is_alive())
           if t1.is_alive():
               t1.join()

       multi_thread()
       print('主线程执行结束')
     #+END_SRC

     #+RESULTS:
     #+BEGIN_SRC org
     - >>> >>> ... ... ... >>> ... ... ... ... ... ... ... >>> 子线程是否存活： True
     - 子线程执行结束。
     - 主线程执行结束
     #+END_SRC

**** name
     用来辨别线程的字符串，没有实际含义。多个线程可有相同名字。初始名
     字由构造器自动生成。
**** getName()
**** setName()
     name属性的旧API。
**** ident
     “线程身份”，如果线程还没启动则为None，如果已启动则为非0整数。当一
     个线程退出，另一个线程启动时，此“身份”可能被重新利用。即使线程退
     出后，也可获取。

**** is_alive()
     如果线程为“存活”状态，则返回True。

     在run()方法开始执行前，run()方法结束前，都返回True。模块函数
     enumerate()返回内容只含当前“存活”线程。

**** daemon
     表示是否为“后台线程”的布尔值。如果赋值，则需在调用start()方法前，
     否则抛出RuntimeError错误。默认继承于创建该线程是否为“后台线程”，
     主线程不为“后台线程”，所以在主线程中创建的线程默认daemon=False。

     当没有非“后台线程”存活时，整个Python程序退出。

**** isDeamon()
**** setDaemon()
     daemon属性的旧API。

** Lock对象

   #+BEGIN_QUOTE
   [[http://www.cszhi.com/20130528/python-threading.html][@cszhi]] ：使用“互斥锁”可保护多个线程访问的公共资源，使各线程不抢夺。
   #+END_QUOTE

   一个原始的lock为一个同步原始，当锁定后，不属于任何一个特定线程。在
   Python中，此类型为目前最底层的同步原始，由_thread扩展模块直接实
   现。？？？

   一个原始lock有两种状态：“locked（锁定）”和“unlocked（去锁定）”。有
   两个基本方法：acquire()和release()。当“去锁定”后，使用acquire()可将
   状态改变为“锁定”状态，并立即返回。当“锁定”后，acquire()会使当前线程
   一直阻塞，直到另一个线程中调用release()方法，将其“解锁”，然后可重新
   调用acquire()将其“锁定”并返回。release()方法只能在“锁定”状态下调用，
   否则抛出RuntimeError错误。

   支持上下文管理器协议。

   当在多个线程中调用acquire()锁定线程后，再调用release()方法时只能解
   锁一个线程；具体解锁哪一个根据Python实现不定。

*** class threading. *Lock*
    实现原始lock对象的类。当对某个线程锁定后，直到调用release()才释放
    （任何线程可操作此释放行为）。

**** acquire(blocking=True,timeout=-1)
     获取锁，阻塞或非阻塞。

     当blocking参数设置为True时（默认），阻塞直到解锁，然后设置为“锁定”状
     态返回True。？？？

     当blocking参数为False时，不阻塞。？？？

     当使timeout参数为浮点数正数时，阻塞对多timeout指定的秒数，期间不
     能获取“锁”。如果timeout参数为-1，表示无限期等待。当bloking参数为
     False时，不得指定timeout参数。

     如果成功获取“锁”，返回True，否则返回False（如超过timeout指定时间）。

**** release()
     释放“锁”，可在任何线程中调用，不必是获取“锁”的那个线程。

     当“锁”被锁定时，将其解锁。如果多个线程正阻塞等待解锁，只能解锁其
     中一个。

     当在没锁定的“锁”上调用时，抛出RuntimeError错误。

     无返回值。

** Rlock对象

   #+BEGIN_QUOTE
   [[http://python.jobbole.com/81546/][@伯乐在线]] Rlock与lock的主要区别为：Rlock允许在同一线程中多次被
   acquire()，Lock则不允许。使用Rlock时，acquire()和release()应成对出
   现，才能释放所有“锁”。
   #+END_QUOTE

   可重入锁，“同步原始（synchronization primitive）”，可被单个线程获取
   多次。使用“拥有者线程”和“递归层级”的概念，以及lock的锁定/解锁状态。
   在锁定状态下，一些线程拥有锁；在解锁状态下，没有线程拥有锁。

   如需锁定锁，线程可调用acquire()方法，当线程拥有锁后就返回。如需解锁
   锁，线程可调用release()方法。acquire()/release()调用对可嵌套，最外
   层release()可将锁设置为解锁状态，使其线程可调用acquire()获取锁。

   可重入锁支持上下文管理协议。

*** class threading. *Rlock*
    实现可重入锁对象的类。可重入锁可被获取他的线程释放。当某个线程获取
    一个可重入锁后，同一个线程可在不阻塞的情况下再次获取，且该线程需在
    每次获取后将其释放。

    Rlock实际上为工厂函数，返回当前平台上效率最高的具体Rlock类（返回实
    例）。

**** acquire(blocking=True,timeout=-1) ？？？
     获取锁，阻塞或非阻塞。

     当不带任何参数时：如果当前线程已经拥有锁，则增加一个递归层级，立
     即返回；如果锁被另一个线程拥有，阻塞直到该锁被释放。一旦锁被释放
     （不被另一个线程拥有），立即获得所有权，增加一个递归层级并返回。
     如果有多个线程等待锁被释放，则每次释放后只有一个线程能够获取锁，
     无返回值。

     当blocking参数为True时（默认），与不带参数时相同，并返回True。

     当blocking参数为False时，不阻塞。但如果调用后造成阻塞，则立即返回
     False；否则与不带任何参数相同，并返回True。

     如果指定表示秒数的浮点数参数timeout，则阻塞最多timeout指定时长，
     期间其他线程不能获取锁。如果成功获取锁，返回True，如果超过timeout
     时长，则返回False。

**** release()
     释放锁，将递归层级减少一层。如果减少至0，则此时锁不被锁定（不被任
     何线程拥有），如果任何其他线程线程正被阻塞等待解锁，则只允许其中
     一个获取锁。如果减少后还不为0，则锁保持锁定，并且调用此方法的线程
     所有。

     只有在拥有锁的线程中才能调用。当锁没被锁定时调用此方法抛出
     RuntimeError错误。

     无返回值。
** Condition对象
*** class threading. *Condition* (lock=none)
    实现条件变量对象的类。一个条件变量可使一个或多个线程等待，直到接到
    另一个线程的通知。

    如果lock参数不为None，则需为Lock或Rlock对象，用作底层的锁。否则自
    动创建Rlock对象作为底层锁。

**** acquire(*args)
     获取锁。此方法调用底层锁的acquire()方法，返回该方法的返回值。
**** release()
     释放锁。此方法调用底层锁的release()方法，无返回值。

**** wait(timeout=None)
     等待（阻塞）直到接到通知或超过timeout直到时间。如果在没有获取锁的
     线程中调用，抛出RuntimeError错误。

     此方法释放底层锁，然后阻塞直到被同一条件变量，在另一个线程中调用
     notify()或notify_all()方法唤醒，或超过timeout时长。一旦唤醒或超过
     timeout时长，重新获得锁并返回。

     timeout参数需为指定秒数的浮点数。

     当底层锁为Rlock对象时，释放锁时，并不是使用Condition对象自身的
     release()方法，因为如果被多次递归获取，没有办法释放锁。实际上，使
     用Rlock对象的release()方法。这样，即时锁被多次递归获取，也可释放。
     重新获取锁时，再使用Rlock的接口恢复。

     除非指定timeout参数并超时，此方法始终返回True。

**** wait_for(predicate,timeout=None)
     等待直到predicate可调用对象返回True。可指定timeout参数限定等待时
     长。

     相当于重复调用wait()直到predicte返回值为True，或超时。返回值为最
     后一次调用predicate的返回值，如果超时返回False。

     如果忽视timeout参数功能，此方法大致等价于：

     #+BEGIN_EXAMPLE python
       while not predicate():
             cv.wait()
     #+END_EXAMPLE

     所以，与wait()的规则相同：当调用此方法时，必须拥有锁，然后返回后
     在重新获取锁。predicate在拥有锁时执行。

**** notify(n=1)
     默认情况下，唤醒使用相同条件变量等待的线程。如果调用线程当时不拥
     有锁，则抛出RuntimeError错误。

     此方法可唤醒使用相同条件变量等待的最多n个线程；如果没有线程处于等
     待状态，相当于“空操作”。

     目前的实现中，如果至少有n个线程正在等待，则会准确唤醒n个线程。然
     而，依赖此特性并不安全。以后的实现可能会唤醒多于n个的线程。

     #+BEGIN_QUOTE
     *注意* ： 直到可重新获取锁前，被唤醒的线程并不是真正从Condition对
     象的wait()方法返回。因为notify()并没有释放锁，而是调用线程。
     #+END_QUOTE

**** notify_all()
     唤醒所有使用该条件变量等待的线程。类似于notify()方法，不过是将所
     有等待的线程唤醒。如果调用线程当时不拥有锁，则抛出RuntimeError错
     误。

** Semaphore（信号量）对象
   计算机科学中最早的同步原始之一。

   信号量管理一个内部计数器，调用acquire()方法后减一，调用release()方
   法后加一。由于计数器永远不会小于0，所以当调用acquire()时，如果发现
   计数器值为0，则等待直到其他线程调用release()方法。

   支持with上下文管理器。

*** class threading. *Semaphore* (value=1)
    实现信号量对象的类。管理一个内部计数器。

    value：内部计数器的初始值，默认为1，如果指定值小于0，则抛出
    ValueError错误。

**** acquire(blocking=True,timeout=None)
     获取信号量。

     当不带参数时：如果内部计数器大于0，则将计数器减一并立即返回。如果
     为0，则阻塞直到其他线程调用release()方法将计数器调用加一。如果多
     次调用acquire()后阻塞，调用一次release()只能唤醒单个，且内部实现
     随机选择。如果不阻塞则返回True。

     blocking为False时：不阻塞，房blocking为True时如果阻塞则返回False；
     否则与不带任何参数相同。

     timeout不为None时：阻塞最多指定秒数。如果不带参数不会阻塞，则返回
     True，如果在指定时间内不能获取则返回False。

**** release()
     释放信号量，将内部计数器增加1。如果当前计数器为0，且其他线程正等
     待，则唤醒其中一个等待的线程。

*** class threading. *BoundedSemaphore* (value=1)
    实现“有限信号量”（Bounded Semaphore）的类。与Semaphore不同之处为：
    当调用release()方法将内部计数器加一后，如果超过value参数指定的初始
    值，则抛出ValueError错误。

*** Semaphore举例
    信号量常用于守护能力有限的资源访问，如数据库服务器。在任何资源大小
    固定的情况下，需使用“有限信号量”。在启动任何工作线程前，需在主线程
    中初始化信号量。

    #+BEGIN_EXAMPLE python
      max_connections = 5
      # ...
      pool_sema = BoundedSemaphore(value=max_connections)
    #+END_EXAMPLE

    一旦启动，工作线程如需访问数据库服务器，可调用信号量对象的
    release()和acquire()方法。

    #+BEGIN_EXAMPLE python
      with pool_sema:
           conn = connectdb()
           try:
              # ... 使用数据库连接
           finally:
              conn.close()
    #+END_EXAMPLE

    使用“有限信号量”，可防止无意间release()调用次数超过acquire()调用次
    数的情况。

** Event对象
   进程间最简单的交流机制：一个线程设置时间信号，其他线程等待该信号。

*** class threading. *Event*
    实现Event实例的类。

    一个Event对象管理内部的flag，此flag可调用set()方法设置为True，使用
    clear()方法设置为False。调用wait()方法会阻塞至此flag值为True为止。
    flag初始值为False。

**** is_set()
     当且仅当flag为True时返回True。
**** set()
     设置flag为True，在调用wait()方法的线程中，当flag为True时不再阻塞。

**** clear()
     设置flag为False，在调用wait()方法的线程中，直到再次调用set()方法
     前会一直阻塞。

**** wait(timeout=None)
     在线程中调用后，阻塞直到flag为True。如果本来就为True，则立即返回。
     否则阻塞直到另一个线程调用set()方法将flag设置为True，或阻塞超过
     timeout指定秒数。

     如果设置timeout参数，须为指定秒数的浮点数。

     不管在阻塞前还是阻塞后，只要flag为True，则此方法返回True。所以如
     果没有指定timeout参数，此方法始终返回True。

** Timer对象
   直到经过指定时长时才执行。Timer为Thread子类，所以也相当于创建一个线
   程。

   与Thread()实例相同，也是调用start()方法激活线程。可在等待过程中，调
   用cancel()方法停止。 *实际等待时长可能不与指定时长完全相等。*

   比如：

   #+BEGIN_EXAMPLE python
     from threading import Timer

     def hello():
         print('hello, world')

     t = Timer(2, hello)
     t.start()
   #+END_EXAMPLE

*** class threading. *Timer* (interval,function,args=None,kwargs=None)
    创建一个计时器，在interval时长后执行function函数，函数参数args和
    kwargs参数执行。args和kwargs参数的默认值分别为空链表和空字典。

**** cancel()
     停止计时器，取消将执行的函数。仅在等待过程中有效。

** Barrier对象
   为多个需互相等待的线程提供同步原始。每个需互相等待的线程中调用
   wait()方法，阻塞直到调用次数达到指定个数，此时所有线程同步释放。

   对于相同数量的线程，Barrier对象可重复使用。

   如下面同步启动客户端和服务器的例子：

   #+BEGIN_SRC python :session
     import time
     from threading import Barrier, Thread

     b = Barrier(2)

     def server():
         b.wait()
         print('server启动')

     def client():
         b.wait()
         print('client启动')

     server_td = Thread(target=server)
     client_td = Thread(target=client)

     server_td.start()
     time.sleep(0.1)
     print('客户端启动启动前')
     client_td.start()
   #+END_SRC

   #+RESULTS:
   #+BEGIN_SRC org
   - >>> >>> >>> >>> >>> ... ... ... >>> >>> ... ... ... >>> >>> >>> >>> >>> >>> >>> 客户端启动启动前
   - client启动
   - server启动
   #+END_SRC

*** class threading. *Barrier* (parties,action=None,timeout=None)
    创建Barrier对象，使用parties参数指定等待线程个数。

    action：可调用对象（默认为None），当所有等待线程同步释放时，由其中
    一个调用。

    timeout：如果wait()方法中的timeout参数为None，则使用此参数值作为默
    认值。

**** wait(timeout=None)
     在需等待的线程中调用。当调用此方法的线程个数达到parties参数指定个
     数时，这些线程同步释放。

     timeout：当提供时，覆盖构造器的timeout参数。

     返回值为0到parties-1间的整数，表示还有多少个线程需调用此方法才能
     同步释放。可用于在指定线程中执行特殊行为：

     #+BEGIN_EXAMPLE python
       i = barrier.wait()
       if i == 0:
          # 只需最后一个线程执行的内容->
          print('这是最后一个需等待的线程')
     #+END_EXAMPLE

     如果构造器中指定action参数，则所有线程同步释放前其中一个会调用该
     参数值。如果此调用抛出错误，则Barrier对象被置于损坏状态。

     如果等待时长超过timeout指定时长，则Barrier对象被置于损坏状态。

     当有其他线程处理等待状态时，如果Barrier对象已损坏或被重置，调用此
     方法可能抛出BrokenBarrierError错误。

**** reset()
     将Barrier对象置于初始状态。任何正等待的线程将接受一个
     BrokenBarrierError错误抛出。

     Note that using this function may can require some external
     synchronization if there are other threads whose state is
     unknown. If a barrier is broken it may be better to just leave it
     and create a new one.

**** abort()
     将Barrier对象置于损坏状态。此后再调用wait()方法将失败，并抛出
     BrokenBarrierError错误。

     比如，如果其中一个线程需放弃执行，防止应用锁死。？？？

     更好的方法时在其中一个线程中调用wait()方法时指定timeout参数。？？？

**** parties
     总共需等待的线程个数。
**** n_waiting
     当前等待的线程个数。
**** broken
     如果为损坏状态，返回True。
*** exception threading. *BrokenBarrierError*
    RuntimeError子类，当Barrier对象重置或损坏时抛出。
** 在with上下文管理器中使用locks、conditions和semaphores
   本模块中只要有acquire()和release()方法的对象，都可用在with上下文管
   理器中。进入时执行release()，退出时执行release()。所以：

   #+BEGIN_EXAMPLE python
     with some_block:
          # do something...
   #+END_EXAMPLE

   等价于：

   #+BEGIN_EXAMPLE python
     some_lock.acuqire()
     try:
         # do something...
     finally:
         some_lock.release()
   #+END_EXAMPLE

   目前，实例支持上下文管理器的类有：Lock、Rlock、Condition、Semaphore
   和BoundedSemaphore。

* multiprocessing - 以进程为基础的并发
** 简介
   multiprocessing模块使用与threading模块类似的API启动新进程。此模块支
   持本地和远程并发，使用子进程替换线程，有效回避了Global Interpreter
   Lock（GIL，全局解释器锁），从而可充分发挥多个处理器的功能。在Unix和
   Windows中都可使用。

   multiprocessing模块还实现了threading模块没有的API，其中最主要为Pool
   类，可将数据分发到多个进程中，同步执行输入数据。如：

   #+BEGIN_SRC python :session
     from multiprocessing import Pool

     def foo(x):
         return x * x

     if __name__ == '__main__':
         with Pool(5) as p:
             print(p.map(foo, [1, 2, 3]))
   #+END_SRC

   #+RESULTS:
   #+BEGIN_SRC org
   - >>> ... ... >>> ... ... ... [1, 4, 9]
   #+END_SRC

*** Process类
    类似于threading.Thread类，multiprocessing.Process创建新Process对象，
    并使用start()方法激活新进程：

    #+BEGIN_SRC python :session
      from multiprocessing import Process

      def f(name):
          print('hello', name)

      if __name__ == '__main__':
          p = Process(target=f, args=('bob',))
          p.start()
          # p.join()
          print('主进程执行结束')
    #+END_SRC

    #+RESULTS:
    #+BEGIN_SRC org
    - >>> ... ... >>> ... ... ... ... ... 主进程执行结束
    - hello bob
    #+END_SRC

    如需获取进程的ID，可结合os模块使用：

    #+BEGIN_SRC python :session
      import os
      from multiprocessing import Process

      def info(title):
          print(title)
          print('模块名称：', __name__)
          print('父进程：', os.getppid())
          print('当前进程：', os.getpid())

      def f(name):
          info(name)
          print('hello', name)

      if __name__ == '__main__':
          info('主进程')
          processes = []
          for i in range(2):
              p = Process(target=f, args=('claudio',))
              processes.append(p)
              p.start()
    #+END_SRC

    #+RESULTS:
    #+BEGIN_SRC org
    - >>> >>> >>> ... ... ... ... ... >>> >>> ... ... ... >>> >>> ... ... ... ... ... ... ... 主进程
    - 模块名称： __main__
    - 父进程： 1462
    - 当前进程： 2262
    - claudio
    - 模块名称： __main__
    - 父进程： 2262
    - 当前进程： 6005
    - hello claudio
    - claudio
    - 模块名称： __main__
    - 父进程： 2262
    - 当前进程： 6006
    - hello claudio
    #+END_SRC

    为何需使用if __name__ == '__main___'，可参考后面的“编程指南”章节。

*** 上下文和启动方法
    根据所在平台不同，multiprocessing模块支持下面3中进程启动方法：

    - spawn：

      父进程启动一个全新的Python解释器。子进程仅继承足够运行新进程
      run()方法的资源。不继承不需要的文件描述符和“句柄（handle）”。较
      后面两种方法相当慢。

      Unix和Windows中都可使用，为Windows的默认方法。

    - fork：

      父进程使用os.fork()方法fork Python解释器。当子进程刚启动时，与父
      进程完全相同。父进程的所有资源都被子进程继承。fork多线程进程会出
      现问题。？？？

      仅在Unix中可用。Unix中的默认方法。

    - forkserver：

      使用此方法后，会开启一个服务器进程。此后，当需创建进程时，父进程
      会连接服务器并请求fork一个新进程。服务器只是一个单线程，可安全使
      用os.fork()方法。不继承非必须资源。

      在支持Unix管道间可传递文件描述符的Unix系统中有效。

    在Unix中使用spawn和forkserver方法启动进程时，还会同时启动一个“信号
    量（semaphore）”跟踪进程，用于跟踪子进程创建的未连接命名信号量。当
    所有进程退出时，信号量跟踪器删除所有未连接信号量连接。通常不会有未连
    接的命名信号量，但当子进程被信号杀死，则会出现“泄露的”信号量。（删
    除未连接信号量非常有必要，因为系统只允许有限个数，且除非重启，不会
    自动删除。）

    如需改变启动方法，只能在if __name__ == '__main__'语句中调用
    set_start_method()方法：

    #+BEGIN_EXAMPLE python
      import multiprocessing as mp

      def foo(q):
          q.put('hello')

      if __name__ == '__main__':
          mp.set_start_method('spawn')
          q = mp.Queue()
          p = mp.Process(target=foo, args=(q,))
          p.start()
          print(q.get())
          p.join()
    #+END_EXAMPLE

    同一个程序中只允许调用一次set_start_method()方法。

    不过，可使用get_context()方法获取上下文对象。上下文对象具有
    multiprocessing模块相同的API，可实现单个程序中使用多种启动方法。

    #+BEGIN_EXAMPLE python
      import multiprocessing as mp

      def foo(q):
          q.put('hello')

      if __name__ == '__main__':
          ctx = mp.get_context('spawn')
          q = ctx.Queue()
          p = ctx.Process(target=foo, args=(q,))
          p.start()
          print(q.get())
          p.join()
    #+END_EXAMPLE

    不过，一个上下文中的特点对象可能不与另一个上下文兼容。比如，使用
    fork方法启动进程中创建的“锁”不能用于使用spawn或forkserver方法创建
    的进程中。

    如果某个库需使用特定启动方法，最好使用get_context()。

*** 进程间交换数据对象
    multiprocessing模块提供下面两种方法来实现进程间通信：

**** 消息队列
     Queue类，基本上为queue.Queue类的克隆。如：

     #+BEGIN_EXAMPLE python
       from multiprocessing import Process, Queue

       def f(q):
           q.put([42, None, 'hello'])

       if __name__ == '__main__':
           q = Queue()
           p = Process(target=f, args=(q,))
           p.start()
           print(q.get())
           p.join()
     #+END_EXAMPLE

     消息队列在线程和进程中都可安全使用。

**** 管道
     Pipe()函数返回一对连接对象，它们被被管道连接（默认为双向）。如：

     #+BEGIN_EXAMPLE python
       from multiprocessing import Pipe, Process

       def f(conn):
           conn.send([42, None, 'hello'])
           conn.close()

       if __name__ == '__main__':
           parent_conn, child_conn = Pipe()

           p = Process(target=f, args=(child_conn,))
           p.start()
           print(parent_conn.recv())
           p.join()
     #+END_EXAMPLE

     Pipe()返回的一对连接对象表示管道两端。每个都想都有send()和recv()
     等方法。如果多个进程（或线程）尝试同时在管道的同一端读/写数据，可
     能会造成数据毁坏；同时读/写不同端则不会。

*** 进程间同步
    multiprocessing模块拥有threading模块的所有同步原始。比如，可利用
    “互斥锁”保证某个时刻只有单个进程输出内容：

    #+BEGIN_EXAMPLE python
      from multiprocessing import Lock, Process

      def f(l, i):
          with l:
              print('hello world', i)

      if __name__ == '__main__':
          lock = Lock()
          for num in range(10):
              Process(target=f, args=(lock, num)).start()
    #+END_EXAMPLE

    如果不使用“互斥锁”，输出顺序不定。

*** 进程间分享状态
    正如前面所言，并发编程时最好尽量避免状态分享。使用多进程时尤是如此。

    不过，如果真正需要共享数据时，可使用multiprocessing模块提供的下面
    两种方法：

**** 内存共享
     可利用Value和Array将共享数据存储在内存中。比如：

     #+BEGIN_EXAMPLE python
       from multiprocessing import Array, Process, Value

       def f(n, a):
           n.value = 3.14
           for i, _ in enumerate(a):
               a[i] = -a[i]

       if __name__ == '__main__':
           num = Value('d', 0.0)
           arr = Array('i', range(10))

           p = Process(target=f, args=(num, arr))
           p.start()
           p.join()

           print(num.value)
           print(arr[:])
     #+END_EXAMPLE

     输出结果为：

     #+BEGIN_EXAMPLE python
       3.14
       [0, -1, -2, -3, -4, -5, -6, -7, -8, -9]
     #+END_EXAMPLE

     'd'和'i'分别为num和arr用于array模块的类型代码：'d'表示双精度浮点
     数，'i'表示整数。此类共享数据在线程和进程中都可安全使用。

     如需更灵活使用贡献内存，可使用multiprocessing.sharedctypes模块，
     支持在共享内存中创建任意C类型数据。

**** 服务器进程
     Manager()返回的管理器对象控制单个服务器进程，该进程中用于持有的Python
     对象，并允许其他进程使用代理操作。

     管理器对象支持的类型有：list、dict、Namespace、Lock、Rlock、
     Semaphore、BoundedSemaphore、Condition、Event、Barrier、Queue、
     Value和Array。比如：

     #+BEGIN_EXAMPLE python
       from multiprocessing import Manager, Process

       def f(d, l):
           d[1] = '1'
           d['2'] = 2
           d[0.25] = None
           l.reverse()

       if __name__ == '__main__':
           with Manager() as manager:
               d = manager.dict()
               l = manager.list(range(10))

               p = Process(target=f, args=(d, l))
               p.start()
               p.join()

               print(d)
               print(l)
     #+END_EXAMPLE

     输出结果为：

     #+BEGIN_EXAMPLE python
       {0.25: None, 1: '1', '2': 2}
       [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
     #+END_EXAMPLE

     由于可支持各种类型的数据，服务器进程管理器比内存共享更灵活。且当
     个管理器可被不同电脑上的进程通过网络共享。不过，速度不及内存共享。

*** 将多个工作单元进程作为一个整体
    类Pool表示多个进程工作单位组成的进程池，可将多个任务以不同方式分配
    给多个进程工作单位。如：

    #+BEGIN_EXAMPLE python
      import os
      import time
      from multiprocessing import Pool, TimeoutError

      def f(x): return x * x

      if __name__ == '__main__':
          # 开启4个工作单位进程
          with Pool(processes=4) as pool:
              # 打印"[0, 1, 4,...81]"
              print(pool.map(f, range(10)))

              # 打印上一例的所有结果，不过以任意顺序
              for i in pool.imap_unordered(f, range(10)):
                  print(i)

              # 同步执行“f(20)”
              res = pool.apply_async(f, (20,))  # 仅在“单个”进程中执行
              print(res.get(timeout=1))        # 结果：400

              # 同步执行os.getpid()
              res = pool.apply_async(os.getpid, ())  # 仅在“单个”进程中执行
              print('当前进程ID：', os.getpid())
              print(res.get(timeout=1))             # 打印该进程ID

              # 开启多个同步计算，”可能”使用多进程
              multi_results = [pool.apply_async(os.getpid, ()) for i in range(4)]
              print([res.get(timeout=1)for res in multi_results])

              # 让单个工作单位休眠10秒
              res = pool.apply_async(time.sleep, (10,))
              try:
                  print(res.get(timeout=0.2))
              except TimeoutError:
                  print('多进程中有某个进程超时')

              print('某个进程超时时，其他工作单位任在继续执行。')

          # 上下文管理器结束执行后，进程池也被关闭
          print('进程池关闭，不再有效。')
    #+END_EXAMPLE

    #+BEGIN_QUOTE
    *注意* ： 进程池的方法只能在创建该进程池的进程中调用。

    *注意* ：只能在if __name__ == '__main__'语句中使用进程池。（不能在
    解释器中使用。）
    #+END_QUOTE

** 参考
*** Process类和错误抛出
    multiprocessing模块的接口几乎同threading模块的相同。
**** multiprocessing. *Process* (group=None,target=None,name=None,args=(),kwargs={},*,daemon=None)
     Process对象表示在新进程中的执行活动。Process类具有
     threading.Thread类的所有等价方法。

     构造器中只能使用关键字参数。

     group：需为None，仅为兼容threading.Thread的API。

     target：可调用对象。被Process实例的run()方法激活。

     name：进程名称。

     args：元组，作为target可调用对象的参数。

     kwargs：字典，作为target可调用对象的关键字参数。

     daemon：设置进程是否为后台执行。默认为None，表示继承于创建此进程
     的进程。

     *如果子类重置构造器，需首先声明Process.__init__()。*

***** 与Thread实例相同的方法属性
****** run()
       表示进程获得的方法。

       可在子类中重载。标准run()方法会调用构造器中的target参数值，并将
       args和kwargs参数作为参数。

****** start()
       激活进程。

       每个Process实例只能调用一次。安排run()方法在新进程中执行。

****** join([timeout])
       如果可选参数timeout为None（默认），则阻塞至join()方法调用结束；
       如果timeout参数为正数，阻塞最多timeout指定时长。如果执行结束或
       超时，此方法返回值都为None。可使用进程的exitcode属性检查是否执
       行结束。

       可多次调用。

       一个进程不能在自身中调用join()方法，否则会造成锁死。在调用
       start()方法前调用此方法会抛出错误。

****** name
       进程名称。为鉴定进程的字符串，没有实际含义。多个进程可能被赋予
       相同名称。

       初始名称由构造器自动赋予。如果缺省name参数，则赋值
       “Process-N1,N2...Nk”，其中Nk为第k可进程。

****** is_alive()
       返回布尔值表示进程是否存活。

       可大致认为，至start()方法返回，进程执行结束前，进程对象都处于存
       活状态。

****** daemon
       表示进程是否后台执行的布尔值。设置需在调用start()方法前。

       初始值默认继承于创建新进程的进程。

       当进程退出时，会尝试终止其中创建的所有后台子进程。

       后台进程中不允许创建子进程。否则，后台进程的父进程退出后，会让
       其子进程成为孤儿进程。此外， *没有* Unix后台进程或服务，只是所
       有非后台进程退出，所有后台进程都被终止。

***** 与Thread实例不同的方法和属性
****** pid
       返回进程ID。子进程启动前，值为None。
****** exitcode
       子进程退出码。如果子进程还没终止，则为None。如果为负数-N，则表
       示子进程被信号N终止。
****** authkey
       子进程的“授权键（authentication key）”，为bytes对象。

       当multiprocessing模块初始化后，会使用os.urandom()为主进程赋值随
       机字符串。

       当Process对象创建时，继承父进程的授权键。不过，可通过设置authey
       属性更改。
****** sentinel ？？？
       数字，表示一个系统处理对象，当进程结束时转变为“准备”状态。

       如需在使用multiprocessing.connection.wait()时等待多个事件，可使
       用此值。否则，调用join()方法更简单。

       Windows中，可与WaitForSingleObject和WaitForMultipleObjects家族
       API连用；Unix中，为文件描述符，可与select模块连用。

****** terminate()
       终止进程。Unix中，底层为发送SIGTERM信号；Windows中，执行
       TerminateProcess()。注意，退出处理函数和finally语句等不会执行。

       注意：子进程不会被终止，而是成为孤儿进程。

       #+BEGIN_QUOTE
       *警告* ：如果在使用管道或消息队列的进程中调用此方法，可能破坏管
       道或消息队列，使其不能被其他进程使用。同样，如果进程获取了锁或
       信号量，调用此方法可能造成其他进程锁死。
       #+END_QUOTE

***** 其他
      start()、join()、is_alive()、terminate()和exitcode只能在创建该进
      程中的进程中调用。

      Process对象的一些方法举例：

      #+BEGIN_EXAMPLE python
        >>> import multiprocessing, time, signal
        >>> p = multiprocessing.Process(target=time.sleep, args=(1000,))
        >>> print(p, p.is_alive())
        <Process(Process-1, initial)> False
        >>> p.start()
        >>> print(p, p.is_alive())
        <Process(Process-1, started)> True
        >>> p.terminate()
        >>> time.sleep(0.1)
        >>> print(p, p.is_alive())
        <Process(Process-1, stopped[SIGTERM])> False
        >>> p.exitcode == -signal.SIGTERM
        True
        >>> p.exitcode
        -15
      #+END_EXAMPLE

**** exception multiprocessing. *ProcessError*
     multiprocessing模块中所有抛出错误的基础类。
**** exception multiprocessing. *BufferTooShort*
     当调用Connection.recv_bytes_into()时，提供的buffer对象太小不足以
     读取消息时抛出的错误。

     如果e为BufferTooShort的实例，则e.args[0]为以bytes对象表示的消息值。

**** exception multiprocessing. *AuthenticationError*
     当出现授权错误时的抛出。
**** exception multiprocessing. *TimeoutError*
     当所有支持timeout参数的函数，等待超时时的抛出。
*** Pipe和Queue
    当进行多进程编程时，一般采用消息传递实现进程间通信，而避免使用任何
    同步原始，如互斥锁。

    两个进程间进行消息传递可使用Pipe()，多消费者-多生产者模式则使用消
    息队列。

    Queue、SimpleQueue和JoinableQueue为多消费者和多生产者模式的FIFO形
    式队列，基于类queue.Queue。区别在于multiprocessing.Queue缺少
    task_done()和join()方法。

    如果使用JoinableQueue， *必须* 在每获取一个队列元素后调用
    task_done()方法，否则用于计数未完成任务的信号量会持续增加，最终导
    致溢出并抛出错误。

    可利用Managers实现共享队列。

    #+BEGIN_QUOTE
    *注意* ： 当队列请求超时时，multiprocessing模块通畅抛出queue.Empty
    和queue.Full错误。但他们并不存在于multiprocessing模块的命名空间中，
    所以需手动从queue模块中导入。
    #+END_QUOTE

    #+BEGIN_QUOTE
    *注意* ：当一个对象被添加到队列上时，该对象被pickled，后台线程稍后
    将pickled后的数据添加到底层管道中。

    1. 当一个对象被添加到空队列上时，在empty()方法返回False和
       get_nowait()不抛出queue.Empty错误前，可能有极短时长的延迟。
    2. 如果多个进程同时操作队列元素，可能队列接受元素的顺序与预期不同。
       但是，同一个进程添加元素的顺序与添加顺序保持一致。
    #+END_QUOTE

    #+BEGIN_QUOTE
    *警告* ：如果正在操作multiprocessing.Queue的进程被
    Process.terminate()或os.kill()杀死，队列中的数据可能被破坏。也就可
    能造成其他进程再使用这些队列时抛出异常。
    #+END_QUOTE

    #+BEGIN_QUOTE ？？？
    *警告* ：如果一个线程已经向队列中添加元素（没调用
    JoinableQueue.cancel_join_thread），则直到所有缓存元素添加到管道
    中前，该进程不可能退出。

    也就意味着，除非保证所有添加到队列中的元素都被消耗，尝试join此类进
    程可能造成锁死。同样，如果该子进程为非后台进程，当父进程退出时尝试
    join其所有非后台子线程时，可能造成父进程挂起。
    #+END_QUOTE

    队列用于进程间交流用法，可参考后文“举例”中最后一例。

    #+BEGIN_QUOTE
    *注意* ： multiprocessing模块中的队列实现类需操作系统实现共享信号
    量。否则，尝试实例化下面的队列类会抛出ImportError错误。
    #+END_QUOTE

**** multiprocessing. *Pipe* ([duplex])
     返回一对Connection对象（conn1, conn2），表示管道两端。

     duplex：如果为True（默认），则管道具有双向性；如果为False，则
     conn1只能用于接受消息，conn2只能用于发送消息。

**** class multiprocessing. *Queue* ([maxsize])
     利用管道和少量互斥锁/信号量实现的进程共享队列。当一个进程开始向队
     列中添加数据时，会开启一个供给线程，将缓存中的数据对象转移到管道
     中。

     当获取/添加超时时，使用queue模块中的queue.Empty/queue.Full抛出错
     误。

     实现了queue.Qeueu中除task_done()和join()外的所有方法。

***** 与queue.Queue相同的方法
****** qsize()
       返回队列近似值大小。由于所线程/多进程的语义，此数值并不可靠。

       在没有实现sem_getvalue()的Unix系统中，如Mac OS X，可能抛出
       NotImplementedError错误。

****** empty()
       如果队列中没有元素，则返回True。由于所线程/多进程的语义，此数值
       并不可靠。

****** full()
       如果队列中元素个数达到上限，则返回True。由于所线程/多进程的语义，
       此数值并不可靠。

****** put(obj[,block[,timeout]])
       同queue.Queue.put()。

****** put_nowait(obj)
       同queue.Queue.put_nowait()。

****** get([block[,timeout]])
       同queue.Queue.get()。
****** get_nowait()
       同queue.Queue.get_nowait()。

***** queue.Queue中没有的方法
      下面为queue.Queue实例没有的方法，不常使用。
****** close()
       表示当前进程不会再向队列中添加任何数据。一旦将所有缓存数据添加
       到管道中后，后台线程就关闭。当队列被垃圾回收时自动调用。

****** join_thread()
       join后台线程。只能在调用close()方法后调用。会阻塞至后台线程退出，
       保证所有缓存数据都写入管道中。

       默认情况下，如果进程不为创建队列的进程，则在退出时会尝试join后
       台线程。该线程可调用cancel_join_thread()方法使join_thread()不做
       任何操作。

****** cancel_join_thread()
       防止join_thread()方法阻塞。即当进程退出时，防止自动join后台线程。

       此方法命名为allow_exit_without_flush()更确切。可能会导致队列数
       据丢失，一般不会使用。如需当前进程立即退出，不等待队列数据写入
       管道，不关心数据是否丢失时可用。

**** class multiprocessing. *SimpleQueue*
     简化的multiprocessing.Queue，与锁定管道十分接近。

***** empty()
      如果队列为空则返回True。
***** get()
      从队列中移除并返回元素。
***** put(item)
      向item添加到队列中。
**** class multiprocessing. *JoinableQueue* ([maxsize])
     multiprocessing.Queue的子类，添加了task_done()和join()方法。

***** task_done()
      同queue.Queue.task_done()。
***** join()
      同queue.Queue.join()。
*** 其他
**** multiprocessing. *active_children* ()
     返回当前进程中所有存活的子进程对象组成的链表。

     有将所有完成执行的进程join的副作用。
**** multiprocessing. *cpu_count* ()
     返回系统中CPU个数。可能会抛出NotImplementedError错误。

     #+BEGIN_QUOTE
     *另见* ：os.cpu_count()。
     #+END_QUOTE

**** multiprocessing. *current_process* ()
     返回表示当前进程的Process对象。

     类似于threading.current_thread()。
**** multiprocessing. *freeze_support* ()
     仅在Windows中有效，（待续）。
**** multiprocessing. *get_all_start_methods* ()
     返回所有支持的启动方法组成的链表，默认启动方法为第一个元素。

     Windows中，只有'spawn'。

     Unix中，总是有'fork'和'spawn'，其中'fork'为默认值。

     #+BEGIN_SRC python :session
       from multiprocessing import get_all_start_methods
       get_all_start_methods()
     #+END_SRC

     #+RESULTS:
     #+BEGIN_SRC org
     - >>> ['fork', 'spawn', 'forkserver']
     #+END_SRC

**** multiprocessing. *get_context* (method=None)
     返回与multiprocessing模块具有相同API的上下文对象。

     method：如果为None则使用默认上下文。否则须
     为'fork'、'spawn'或'forkserver'。如果不支持指定方法，则抛出
     ValueError错误。

     #+BEGIN_SRC python :session
       import multiprocessing
       len(set(dir(multiprocessing.get_context())) ^ set(dir(multiprocessing)))
     #+END_SRC

     #+RESULTS:
     #+BEGIN_SRC org
     - 42
     #+END_SRC

**** multiprocessing. *get_start_method* (allow_none=False)
     返回用于启动进程的方法名称。

     allow_none：如果启动方法还没被固定，且allow_none参数为False，则启
     动方法固定为默认方法，且返回对应名称；如果启动方法还没被固定，且
     allow_none参数为True，则返回None。

     返回值可以为'fork'、'spawn'、'forkserver'或None。'fork'为Unix系统
     的默认返回值，'spawn'为Windows系统的默认返回值。

     #+BEGIN_SRC python :session
       from multiprocessing import get_start_method
       get_start_method()
       get_start_method(allow_none=True)
     #+END_SRC

     #+RESULTS:
     #+BEGIN_SRC org
     - 'fork'
     - 'fork'
     #+END_SRC

**** multiprocessing. *set_executable* ()
     设置启动子进程时使用的Python解释器路径。（默认为sys.executable）。

     如开启新进程前使用：

     #+BEGIN_EXAMPLE python
       set_executable(os.path.join(sys.exec_prefix, 'pythonw.exe'))
     #+END_EXAMPLE

**** multiprocessing. *set_start_method* (method)
     设置启动新进程的方法。method参数可为'fork'、'spawn'和'forkserver'。

     只能调用一次，且只能在if __name__ == '__main__'语句中调用。

     #+BEGIN_QUOTE
     *注意* ： multiprocessing模块 *没有* 与threading模块对应的方法有：

     - threading.active_count()
     - threading.enumerate()
     - threading.settrace()
     - threading.setprofile()
     - threading.Timer
     - threading.local
     #+END_QUOTE
*** Connection对象
    Connection对象允许发送和接受可pickable的对象和字符串。可被视作使用
    消息连接的“套接口（socket）”。

    连接对象通常使用Pipe()创建——另见“监听和客户端”小节。

**** class multiprocessing. *Connection*
***** send (obj)
      向管道另一端发送数据，另一端需用recv()方法接收。

      obj需为pickable的对象。如果pickle对象对象（约32MB+，由系统决定）
      可能抛出ValueError错误。

***** recv()
      返回管道另一端使用send()方法发送的数据。如果另一端没有发送数据，
      阻塞至有为止。如果另一端没有发送数据且已关闭，抛出EOFError错误。

***** fileno()
      返回管道两端连接对象对应的文件描述符或“句柄（handle）”。
***** close()
      关闭管道任何一端连接对象。

      当连接对象被垃圾回收时，自动调用。
***** poll([timeout])
      返回表示是否有可读取数据的布尔值。

      timeout：如果缺省，则立即返回；如果为正数，则为对多阻塞秒数；如
      果为None，则一直轮询至有数据可读取为止。

      注意，使用multiprocessing.connect.wait()使多个连接对象统一单次轮
      询。？？？

***** send_bytes(buffer[,offset[,size]])
      将“类bytes对象”作为完整消息发送。

      offset：如果指定，从buffer中此参数值指定的位置开始读取。

      size：如果指定，从buffer中读取此参数值指定字节数发送。

      如果pickle对象对象（约32MB+，由系统决定）可能抛出ValueError错误。

***** recv_bytes([maxlength])
      从管道连接另一端发送的数据中读取，并返回bytes数据。如果不能读取
      任何数据，阻塞至有数据读取为止。如果不能读取任何数据，且管道另一
      端已关闭，则抛出EOFError错误。

      maxlength：如果指定，且读取消息数据大小大于该上限，则抛出OSError
      错误，且此管道连接不再可读。

***** recv_bytes_into(buffer[,offset])
      从管道另一端发送的数据读取，将读取内容写入buffer中，返回写入字节
      数。如果不能读取任何数据，阻塞至有数据读取为止。如果不能读取任何
      数据，且管道另一端已关闭，则抛出EOFError错误。

      buffer：须为可写“类bytes对象”。

      offset：如果指定，则写入操作从buffer中此参数指定位置开始。需为非
      负整数，且小于buffer字节数。

      如果buffer太小，不能写入，则抛出BufferTooShort错误，完整的消息内
      容可从抛出对象的e的e.args[0]元素中获取。

      从Python3.3开始：连接对象本身也可通过Connection.send()和
      Connection.recv()在进程中传递。

**** 举例和警告

     #+BEGIN_EXAMPLE python
       >>> from multiprocessing import Pipe
       >>> a, b = Pipe()
       >>> a.send([1, 'hello', None])
       >>> b.recv()
       [1, 'hello', None]
       >>> b.send_bytes(b'thank you')
       >>> a.recv_bytes()
       b'thank you'
       >>> import array
       >>> arr1 = array.array('i', range(5))
       >>> arr2 = array.array('i', [0] * 10)
       >>> a.send_bytes(arr1)
       >>> count = b.recv_bytes_into(arr2)
       >>> arr2
       array('i', [0, 1, 2, 3, 4, 0, 0, 0, 0, 0])
       >>> assert count == len(arr1) * arr1.itemsize
     #+END_EXAMPLE

     #+BEGIN_QUOTE
     *警告* ：Connection.recv()方法会自动unpickle接受数据，除非接收消
     息来自信任的进程，否则会有安全风险。

     所以，如果连接对象不是由Pipe()生成，只能在授权操作后使用recv()和
     send()方法。可参考后面的“授权键”小节。
     #+END_QUOTE

     #+BEGIN_QUOTE
     *警告* ：当正试图对管道进行读写时，如果进程被杀死，则很可能破坏管
     道数据。因为这样就不能判断消息的内容边界。
     #+END_QUOTE

     #+BEGIN_QUOTE
     *注意* ： 连接对象支持with上下文，不过不支持with Pipe() as ...。
     参考后面“监听和客户端”小节。
     #+END_QUOTE

*** 同步原始
    不同于多线程程序，多进程程序中一般没必要使用同步原始。

    可使用Managers创建下面的同步原始。

**** class multiprocessing. *Barrier* (parities[,action[,timeout]])
     克隆自threading.Barrier。

**** class multiprocessing. *BoundedSemaphore* ([value])
     与threading.BoundedSemaphore唯一的不同：

     acquire()方法的第一个参数为block，而非blocking，与Lock.acquire()
     一致。

     #+BEGIN_QUOTE
     *注意* ： 在Mac OS X中，由于系统没有实现sem_getvalue()，所以与
     multiprocessing.Semaphore没有区别
     #+END_QUOTE

**** class multiprocessing. *Condition* ([lock])
     threading.Condition别名。

     lock：如果指定，须为multiprocessing模块中的Lock或Rlock对象。

**** class multiprocessing. *Event*
     threading.Event的克隆。
**** class multiprocessing. *Lock*
     非递归互斥锁对象：类似于threading.Lock。一旦某个线程/进程获取锁后，
     其他线程/进程尝试获取时会阻塞，直到被释放；任何线程/进程都可释放。
     除下面这点外，与threading.Lock完全相同：

     multiprocessing.Lock为工厂函数，返回值为使用默认上下文实例化的
     multiprocessing.synchronize.Lock对象。

     也支持上下文管理器，可用于with语句中。

***** acquire(block=True,timeout=None)
      获取锁，阻塞或不阻塞。

      block如果为True（默认）：阻塞直到互斥锁处于非锁定状态，然后锁
      定互斥锁并返回True。区别于threading.Lock.acquire()的第一个参数
      “blocking”。

      block如果为False：不阻塞。如果互斥锁当前处于锁定状态，返回False；
      如果处于非锁定状态，锁定互斥锁并返回True。

      timeout：如果为非负浮点数，则在获取互斥锁前最多阻塞此参数指定时
      长；如果为负数，则相当于0；如果为None（默认），则无限期等待。而
      threading.Lock.acquire()中的timeout参数只有为-1时才无限期阻塞。

      如果block参数为False，则忽视此参数。

      如果成功获取互斥锁则返回True，超时则返回False。

***** release()
      释放互斥锁。可被任意进程/线程调用，不一定为原获取互斥锁的进程/线
      程。

      当互斥锁处于非锁定状态时，抛出ValueError错误；而调用
      threading.Lock.release()时，如果处于非锁定状态，则抛出
      RuntimeError错误。

**** class multiprocessing. *Rlock*
     递归互斥锁：类似于threading.Lock。递归互斥锁必须由获取它的进程/线
     程释放。进程/线程获取递归互斥锁后，同一个进程/线程可再次获取；释
     放次数需与获取次数相等。

     注意，multiprocessing.Rlock实际上为工厂函数，返回由
     multiprocessing.synchronize.Rlock使用默认上下文实例化的对象。

     支持上下文管理器，可用于with语句中。

***** acquire(block=True,timeout=None)
      获取递归互斥锁，阻塞或非阻塞。

      如果block参数为True：阻塞直到互斥锁处于解锁状态（不被任何进程/线
      程拥有），除非互斥锁的拥有者为当前进程/线程。当前进程/线程获取互
      斥锁拥有权（如果还没有拥有权），且互斥锁的递归层级增加1，并返回
      True。注意，threading.Rlock.acquire()总的第一个参数为“blocking”。

      如果block参数为False：不阻塞。如果互斥锁当前被其他进程/线程拥有，
      则当前进程/线程不获取拥有权，且递归层级不增加，并返回False。如果
      互斥锁不被任何进程/线程拥有，则获取拥有权，互斥锁递归层级加一，
      返回True。

      timeout：与multiprocessing.Lock.acquire()中的timeout参数用法相同。

***** release()
      释放互斥锁，将递归层级减一。如果递归层级变为0，则互斥锁不被任何
      进程/线程拥有，此时如果有进程/线程等待互斥锁，则将所有权转交到其
      中一个中。如果递归层级依然不为0，则还被当前进程/线程拥有。

      仅可在拥有者进程/线程中调用此方法。不在拥有者进程/线程中调用，或
      互斥锁处于非锁定状态，抛出AssertionError错误。而
      threading.Rlock.release()在此情况下抛出RuntimeError错误。

**** class multiprocessing. *Semaphore* ([value])
     类似于threading.Semaphore。

     与threading.Semaphore的区别为，acquire()方法的的第一个参数为block
     而非blocking，与multiprocessing.Lock.acquire()保存一致。

     #+BEGIN_QUOTE
     *注意* ： 在Mac OS X中，不支持sem_timedwait，acquire()时如果指定
     timeout参数，将使用休眠循环模拟。
     #+END_QUOTE

     #+BEGIN_QUOTE
     *注意* ： 如果使用Ctrl-C发送SIGINT信号，当主线程由于调用
     BoundedSemaphore.acquire()、Lock.acquire()、Rlock.acquire()、
     Semaphore.acquire()、Condition.acquire()和Condition.wait()处于阻
     塞状态时，则立即抛出KeyboardInterrupt错误。

     而在threading模块中，处于同样情况的阻塞状态下，会忽视SIGINT信号。
     #+END_QUOTE

     #+BEGIN_QUOTE
     *注意* ：multiprocessing模块中的一些功能需操作系统上有实现信号量
     共享。如果没有实现，则取消multiprocessing.synchronize模块，导入时
     抛出ImportError错误。
     #+END_QUOTE

*** 共享ctypes对象
    可使用子进程可继承的共享内容创建共享数据对象。

**** multiprocessing. *Value* (typecode_or_type,*args,lock=True)
     返回由共享内容分配的ctypes对象。默认情况下，返回值为为同步包装后
     的对象。返回值本身可使用value属性访问。

     typecode_or_type：决定返回对象类型。可为ctypes中的类型，也可为
     array模块中的类型字符。

     *args：作为指定类型的值。

     lock：为True时（默认），为同步访问返回对象新生成一个递归互斥锁。
     如果为multiprocessing.Lock或multiprocess.Rlock，则用于同步访问返
     回对象；如果为False，则对返回对象的访问不会被互斥锁保护，所以成为
     “进程不安全”。

     如+=等对对象进行读写的操作不为“原子操作（atomic）”。如：

     #+BEGIN_EXAMPLE python
       counter.value += 1
     #+END_EXAMPLE

     不能保证自动修改共享对象的值。

     如果内部的互斥锁为递归形式（默认），可改为：

     #+BEGIN_EXAMPLE python
       with counter.get_lock():
            counter.value += 1
     #+END_EXAMPLE

     再如下例，如果不使用get_lock()，则不能保证执行结果正确（即100）：

     #+BEGIN_EXAMPLE python
       from multiprocessing import Process, Value

       def f(counter):
           with counter.get_lock():
               counter.value = 1 + counter.value

       if __name__ == '__main__':
           counter = Value('i', 0)
           processes = []
           for i in range(100):
               p = Process(target=f, args=(counter,))
               processes.append(p)
               p.start()

           for p in processes:
               p.join()

           print(counter.value)
     #+END_EXAMPLE

     注意，lock只能使用关键字参数。

**** multiprocessing. *Array* (typecode_or_type,size_or_initializer,*,lock=True)
     返回共享内存分配的ctypes数组。默认情况下，返回值实际上为同步包装
     后的数组。

     typecode_or_type：决定返回数组中元素的类型。可为ctypes中的类型，
     也可为array模块中指定类型的字符。

     size_or_initializer：如果为正整数，表示数组长度，数组中所有元素初
     始化为0；如果为序列，则用来初始化数组，其长度决定数组长度。

     lock：如果为True（默认），为访问返回对象新创建一个互斥锁（不为递
     归互斥锁）；如果为Lock或Rlock对象，用于同步访问返回对象；日狗为
     False，则对返回对象的访问不自动受互斥锁保护，所以成为“进程不安全”。

     注意，lock只能使用关键字参数。

     注意，可利用ctypes.c_char，结合value和raw属性存储和访问字符串。

**** multiprocessing.sharedctypes模块
     利用multiprocessing.sharedctypes模块，可让共享内存分配数据对象，
     供子进程继承。

***** multiprocessing.sharedctypes. *RawArray* (typecode_or_type,size_of_initializer)
      返回共享内存分配的ctypes数组。

      typecode_or_type：决定返回数组中元素的类型。可为ctypes中的类型，
      也可为array模块中类型字符。

      size_or_initializer：如果为正整数，则表示数组长度，数组元素初始
      化为0；如果为序列，则用来初始化数组，长度由序列长度决定。

      注意：在数组送设置和获取元素可能不为“原子操作”。可使用
      multiprocessing.sharedctypes.Array来保证对元素的访问被使用互斥锁
      自动同步。

***** multiprocessing.sharedctypes. *RawValue* (typecode_or_type,*args)
      返回共享内存分配的ctypes对象。

      typecode_or_type：决定返回对象的类型，可以为ctypes中的类型，也可
      为array模块中的类型字符。

      *args：作为指定类型返回对象的值。

      注意，设置和获取返回对象的值可能不为原子操作。可使用
      multiprocessing.sharedctypes.Value来保证访问被互斥锁同步保护。

      注意，ctypes.c_char拥有value和raw属性，可用来存储和获取字符串。

***** multiprocessing.sharedctypes. *Array* (typecode_or_type,size_if_initializer,*,lock=True)
      在RawRarry()基础上添加了lock参数，根据lock参数值，对返回对象实现
      同步包装，而不是仅返回原始的ctype数组。

      lock：如果为True（默认），创建新互斥锁以同步访问返回对象值；如果
      为Lock或Rlock对象，则用于同步访问返回对象值；如果为False，则对返
      回对象的返回不受互斥锁保护，所以“进程不安全”。

      lock仅能作为关键字参数使用。

***** multiprocessing.sharedctypes. *Value* (typecode_or_type,*args,lock=True)
      在RawValue()基础上添加了lock参数，根据lock参数值，对返回对象实现
      同步包装。

      lock：如果为True（默认），创建新互斥锁以同步访问返回对象值；如果
      为Lock或Rlock对象，则用于同步访问返回对象值；如果为False，则对返
      回对象的返回不受互斥锁保护，所以“进程不安全”。

      lock仅能作为关键字参数使用。
***** multiprocessing.sharedctypes. *copy* (obj)
      复制ctypes对象obj，作为共享内存分配的ctype对象返回。

***** multiprocessing.sharedctypes. *synchronized* (obj[,lock])
      返回经进程安全包装后的ctypes对象obj，使用lock参数指定互斥锁同步
      化访问。

      lock：如果为None（默认），自动创建multiprocessing.Rlock。

      除被包装对象自身的方法外，下面两个方法：

      1. get_obj：返回被包装对象。
      2. get_lock：返回互斥锁对象。

      注意，访问被包装后的对象，比访问原始ctypes对象慢很多。

      支持with上下文管理器。

**** 总结

     下表为共享内存分配的ctypes对象，与普通ctypes对象的语法区别。（其
     中MyStruct为ctypes.Structure的子类）：

     | ctypes           | 使用ctype类型的共享ctypes | 使用array模块类型代码的共享ctypes |
     |------------------+---------------------------+-----------------------------------|
     | c_double(2.4)    | RawValue(c_double,2.4)    | RawValue('d', 2.4)                |
     | MyStruct(4,6)    | RawValue(MyStruct, 4,6)   |                                   |
     | (c_short*7)()    | RawArray(c_short,7)       | RawArray('h',7)                   |
     | (c_int*3)(9,2,8) | RawArray(c_int,(9,2,8))   | RawArray('i',(9,2,8))             |

     下列中利用子进程修改多个ctypes对象：

     #+BEGIN_EXAMPLE python
       from ctypes import Structure, c_double
       from multiprocessing import Lock, Process
       from multiprocessing.sharedctypes import Array, Value

       class Point(Structure):
           _fields_ = [('x', c_double), ('y', c_double)]

       def modify(n, x, s, A):
           with n.get_lock():
               n.value += 2
               x.value += 2
               s.value = s.value.upper()
               for a in A:
                   a.x += 2
                   a.y += 2

       if __name__ == '__main__':
           lock = Lock()
           n = Value('i', 2)

           x = Value(c_double, 2.0, lock=lock)
           s = Array('c', b'hello world', lock=lock)
           A = Array(Point, [(1.875, -6.25), (-5.75, 2.0), (2.375, 9.5)], lock=lock)

           processes = []
           for i in range(1000):
               p = Process(target=modify, args=(n, x, s, A))
               processes.append(p)
               p.start()

           for p in processes:
               p.join()

           print(n.value)
           print(x.value)
           print(s.value)
           print([(a.x, a.y) for a in A])
     #+END_EXAMPLE

     输出结果为：

     #+BEGIN_EXAMPLE python
       2002
       2002.0
       b'HELLO WORLD'
       [(2001.875, 1993.75), (1994.25, 2002.0), (2002.375, 2009.5)]
     #+END_EXAMPLE

*** “管理器（Managers）”
    使用管理器可实现不同进程间共享数据，包括通过网络在不同主机间共享数
    据。一个管理器对象控制一个服务器进程，用于管理共享数据对象。其他进
    程可通过代理访问这些共享数据。

**** multiprocessing. *Manager* ()
     返回一个已启动的SyncManager对象，可用于进程间共享数据。返回的管理
     器对象对应有一个已开启的子进程，并拥有创建共享数据对象和返回对象
     代理的方法。

     管理器进程会在垃圾回收时，或父进程退出时关闭。

**** multiprocessing.managers模块
     所有的管理器类都定义在multiprocessing.managers模块中。

***** multiprocessing.managers. *BaseManager* ([address[,authkey]])
      创建BaseManager对象。

      一旦创建，需调用start()，或get_server().server_forever()保证管理
      器对象启动管理器进程。

      address：管理器进程对象监听地址。如果为None，则可监听任意地址。

      authkey：授权键，用于检查访问服务进程是否合法。如果为None，则使
      用current_process().authkey。如果指定，须为bytes对象。

      支持with上下文管理器，__enter__()开启服务器进程（如果为开启），
      返回管理器对象，__exit__()调用shutdown()方法。

****** start([initializer[,initagrs]])
       为启动管理器启动一个子进程。

       initializer：如果不为None，则子进程启动时会调用
       initializer(*initargs)。

****** get_server()
       返回表示管理器对象控制的实际Server对象。Server对象支持
       server_forever()方法，且有address属性。

       #+BEGIN_EXAMPLE python
         >>> from multiprocessing.managers import BaseManager
         >>> manager = BaseManager(address=('', 50000), authkey=b'abc')
         >>> server = manager.get_server()
         >>> server.address
         ('0.0.0.0', 50000)
         >>> server.serve_forever()
       #+END_EXAMPLE

****** connect()
       将本地管理器对象与远程管理器进程连接：

       #+BEGIN_EXAMPLE python
         >>> from multiprocessing.managers import BaseManager
         >>> manager = BaseManager(address=('127.0.0.1', 50000), authkey=b'abc')
         >>> manager.connect()
       #+END_EXAMPLE

       #+BEGIN_QUOTE
       *注意* ： 需在指定地址上有远程管理器对象服务运行，否则抛出
       ConnectionRefusedError错误。
       #+END_QUOTE

****** shutdown()
       停止被管理器使用的进程。只对使用start()方法启动的服务器进程有效。

       可多次调用。

****** register(typeid[,callable[,proxytype[,exposed[,method_to_typeid[,create_method]]]]])
       类方法，用于在管理器类上注册数据类型或可调用对象。

       typeid：“类型识别符”，用于识别共享数据对象的类型，须为字符串。

       callable：可调用对象，用于创建typeid识别符对应的对象。如果管理
       器对象用于使用connect()方法连接服务器对象，或create_method参数
       为False，则可缺省。

       proxytype：BaseProxy子类，用于为共享数据对象创建与typeid对应的
       代理。如果为None，则自动创建代理类。

       exposed：指定允许使用BaseProxy._callmethod()访问的typeid指定对
       象的方法。如果为None，且proxytype._exposed_存在，则使用
       proxytype._exposed_。如果不指定，则可访问共享对象的所有“公共方
       法（任何具有__call__()方法的属性，且不以“_”开头）”。

       method_to_typeid：指定暴露方法（返回代理对象）返回值的类型。为
       名称映射typeid字符串。如果method_to_typeid为None，且
       proxytype._method_to_typeid_存在，则使用之。无关方法名不存在于此映
       射的key中，或映射为None，则方法返回的对象被值复制。？？？

       create_method：决定是否创建typeid对应的方法，用于告知服务器进程
       创建新共享对象，并返回其代理。默认为True。

****** address
       属性，管理器对象的地址。

***** multiprocessing.managers. *SyncManager*
      BaseManager子类，用于同步化进程。此类的实例为
      multiprocessing.Manager()的返回值。

      且支持创建共享链表和字典。

****** Barrier(parties[,action[,timeout]])
       创建共享threading.Barrier对象，并返回其代理。

****** BoundedSemaphore([value])
       创建共享threading.BoundedSemaphore对象，并返回其代理。

****** Condition([lock])
       创建共享threading.Condition对象，并返回其代理。

       lock：如果指定，则须为threading.Lock或threading.Rlock对象。

****** Event()
       创建共享threading.Event对象，并返回其代理。

****** Lock()
       创建共享threading.Lock对象，并返回其代理。

****** Namespace()
       创建共享multiprocessing.managers.Namespace对象，并返回其代理。

****** Queue([maxsize])
       创建共享queue.Queue对象，并返回其代理。

****** Rlock()
       创建共享threading.Rlock对象，并返回其代理。

****** Semaphore([value])
       创建共享threading.Semaphore对象，并返回其代理。

****** Array(typecode,sequence)
       创建数组，并返回其代理。
****** Value(typecode,value)
       创建具有可写属性“value”的对象，并返回其代理。

****** dict()
****** dict(mapping)
****** dict(sequence)
       创建共享dict对象，并返回其代理。
****** list()
****** list(sequence)
       创建共享list对象，并返回其代理。

       #+BEGIN_QUOTE
       *注意* ：如果修改字典中mutable类型键值对，或链表中mutable类型的
       元素，不会通过管理器对象向上传播，因为代理不可能知晓其值何时会
       改变。如需修改这些值，可通过重新赋值的方式：

       #+BEGIN_EXAMPLE python
         # 创建代理链表，并添加mutable对象（字典）
         lproxy = manager.list()
         lproxy.append({})

         # 修改该字典
         d = lproxy[0]
         d['a'] = 1
         d['b'] = 2

         # 此时，对d的修改并没同步
         # 但可重新赋值改变代理对象
         lproxy[0] = d
       #+END_EXAMPLE

       #+END_QUOTE

***** multiprocessing.managers. *Namespace*
      可在SyncManager上注册的类型。

      命名空间对象没有公共方法，但有可写属性。使用print打印会显示所有
      属性值得。

      但是，当对命名空间对象使用代理时，以"_"开头的属性会成为代理对象
      的属性，而非被代理对象的属性：

      #+BEGIN_EXAMPLE python
        >>> manager = multiprocessing.Manager()
        >>> Global = manager.Namespace()
        >>> Global.x = 10
        >>> Global.y = 'hello'
        >>> Global._z = 12.3
        >>> print(Global)
        Namespace(x=10, y='hello')
      #+END_EXAMPLE

**** 定制管理器
     创建自己的管理器对象时，可创建BaseManager的子类，并使用类方法
     register()注册新的类型或可调用对象，比如：

     #+BEGIN_SRC python :session
       from multiprocessing.managers import BaseManager

       class MathsClass:
           def add(self, x, y): return x + y
           def mul(self, x, y): return x * y

       class MyManager(BaseManager):
           pass

       MyManager.register('Maths', MathsClass)

       if __name__ == '__main__':
           with MyManager() as manager:
               maths = manager.Maths()
               print(maths.add(4, 3))
               print(maths.mul(4, 3))
     #+END_SRC

     #+RESULTS:
     #+BEGIN_SRC org
     - >>> >>> ... ... ... >>> >>> ... ... >>> >>> >>> >>> ... ... ... ... ... 7
     - 12
     #+END_SRC

**** 使用远程管理器
     可在一个主机上允许管理器服务器，然后在其他主机上创建客户端进行连
     接（假定防火墙允许）。

     运行下面命令为单个共享队列创建服务进程，远程客户端可访问：

     #+BEGIN_EXAMPLE python
       import queue
       from multiprocessing.managers import BaseManager

       queue = queue.Queue()

       class QueueManager(BaseManager):
           pass

       QueueManager.register('get_queue', callable=lambda: queue)
       m = QueueManager(address=('', 50000), authkey=b'abc')
       s = m.get_server()
       s.serve_forever()
     #+END_EXAMPLE

     一个客户端访问管理器服务器并向队列中添加数据：

     #+BEGIN_EXAMPLE python
       from multiprocessing.managers import BaseManager

       class QueueManager(BaseManager):
           pass

       m = QueueManager(address=('localhost', 50000), authkey=b'abc')
       QueueManager.register('get_queue')

       m.connect()
       queue = m.get_queue()
       queue.put('hello')
     #+END_EXAMPLE

     另一个客户端从队列中获取数据：

     #+BEGIN_EXAMPLE python
       from multiprocessing.managers import BaseManager

       class QueueManager(BaseManager):
           pass

       QueueManager.register('get_queue')
       m = QueueManager(address=('localhost', 50000), authkey=b'abc')
       m.connect()
       queue = m.get_queue()
       print(queue.get())
     #+END_EXAMPLE

     输出结果为：

     #+BEGIN_EXAMPLE python
       hello
     #+END_EXAMPLE

*** Proxy对象
    代理对象为引用可能存在于多个进程中对象的对象。共享对象被称为代理对
    象的被引用对象。多个代理对象可能引用同一个共享对象。

    代理对象具有激活被引用对象方法的方法（可能不是被代理对象的全部方
    法）。通畅情况下，可如使用被引用对象一样使用代理对象：

    #+BEGIN_EXAMPLE python
      >>> from multiprocessing import Manager
      >>> manager = Manager()
      >>> l = manager.list([i*i for i in range(10)])
      >>> print(l)
      [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
      >>> print(repr(l))
      <ListProxy object, typeid 'list' at 0x7f4ebe74cdd8>
      >>> l[4]
      16
      >>> l[2:5]
      [4, 9, 16]
    #+END_EXAMPLE

    如上例所示，对代理对象调用str()方法返回表示被代理对象的字符串，而
    调用repr()方法则返回表示代理对象本身的字符串。

    代理对象重要的特征为picklable，从而可在进程中传递。但是，如果如果
    代理对象被发送到管理器的进程，则unpickle时会生成被代理对象本身。如：

    #+BEGIN_EXAMPLE python
      >>> a = manager.list()
      >>> b = manager.list()
      >>> a.append(b) # 此时代理对象a含被代理对象b
      >>> print(a, b)
      [[]] []
      >>> b.append('hello')
      >>> print(a, b)
      [['hello']] ['hello']
    #+END_EXAMPLE

    #+BEGIN_QUOTE
    *注意* ： multiprocessing中的代理对象不支持比较：

    #+BEGIN_EXAMPLE python
      >>> manager.list([1,2,3]) == [1,2,3]
      False
    #+END_EXAMPLE

    可通过复制被代理对象实现比较。

    #+END_QUOTE

**** class multiprocessing.managers. *BaseProxy*
     所有的代理对象都为BaseProxy的子类。

***** _callmethod(methodname[,args[,kwds]])
      调用被代理对象的方法，并返回对应返回值。

      假设proxy为obj的代理对象，则：

      #+BEGIN_EXAMPLE python
        proxy._callmethod(methodname, args, kwds)
      #+END_EXAMPLE

      在管理器的进程中等价于：

      #+BEGIN_EXAMPLE python
        getattr(obj, methodname)(*args, **kwds)
      #+END_EXAMPLE

      返回值为被引用对象方法返回值的副本，或共享对象的新代理——参考
      BaseManager.register()的method_to_typeid参数。

      如果调用方法时抛出错误，则由_callmethod()重新抛出。如果在管理器
      进程中抛出其他错误，则转换为RemoteError并被_callmethod()抛出。

      如果methodname没被暴露，也会抛出错误。

      下例演示了_callmehtod的用法：

      #+BEGIN_EXAMPLE python
        >>> l = manager.list(range(10))
        >>> l._callmethod('__len__')
        10
        >>> l._callmethod('__getitem__', (slice(2, 7),))
        [2, 3, 4, 5, 6]
        >>> l._callmethod('__getitem__', (20,))
        Traceback (most recent call last):
        ...
        IndexError: list index out of range
      #+END_EXAMPLE

***** _getvalue()
      返回被引用对象的副本。

      如果被引用对象unpicklable，则抛出错误。

***** __repr__()
      返回表示代理对象的字符串。
***** __str__()
      返回表示被引用对象的字符串。

**** 清空
     代理对象使用弱引用回调，在垃圾回收时，从拥有被引用对象的管理器进
     程中将自己撤销。

     如果在管理器进程中没有被任何代理对象引用，则被代理对象被删除。

*** “进程池（Pool）”
    可通过类Pool创建进程池，将任务分配给进程池的多个进程。

**** class multiprocessing.pool. *Pool* ([processes[,initilizer[,initagrs[,maxtasksperchild[,context]]]]])
     一个进程池对象控制多个工作进程，可向其中提交任务。支持超时和回调机制异
     步求结果，以及平行映射实现。

     processes：将使用的工作进程数量，如果为None，则使用os.cpu_count()
     的返回值。

     initializer：如果不为None，则每个工作进程启动时将调用
     initializer(*initargs)。

     maxtaskperchild：每个工作进程处理的最多任务数量，如果超出该值，则
     退出工作进程，创建一个全新的工作进程，进而释放没被使用的资源。

     默认为None，表示所有工作进程存活至进程池执行完毕。

     context：指定用于创建工作进程的上下文。一般情况下，使用
     multiprocessing.Pool或multiprocessing.pool.Pool创建进程池时都会合
     理字典设置。

     注意，进程池对象的方法只能在创建该进程池对象的进程中调用。

     #+BEGIN_QUOTE
     *注意* ：进程池中的所有工作进程会存活至进程池队列执行完毕。可通过
     maxtasksperchild参数设置一个工作进程最多处理任务的个数，使单位工
     作进程执行完一定数量任务后释放资源，自动创建新进程。Apache和
     mod_wsgi就采用了同样实现。
     #+END_QUOTE

     支持with山下文管理器，__enter__()方法返回进程池对象，__exit__()方
     法调用terminate()方法。

***** apply(func[,args[,kwds]])
      调用fun，使用参数args和关键字参数kwds。阻塞至调用返回结果。func
      只在进程池的一个工作进程中运行。如果不希望阻塞，可使用
      apply_async()实现平行运行。

***** apply_async(func[,args[,kwds[,callbacks[,error_callback]]]])
      apply()的并行变体。

      callback：如果指定，需为可调用对象，当func调用得出返回值时调用。
      使用func返回结果作为唯一参数。

      error_callback：如果指定，需为可调用对象，当func调用抛出错误时调
      用，接受func抛出的错误对象作为唯一参数。

      以上两个回调函数都不应阻塞，否则处理func返回结果的线程会被阻塞。

***** map(func,iterable[,chunksize])
      内置函数map()的并行版本，不过只支持单个可迭代对象作为参数。阻塞
      至调用得出结果。

      此方法将可迭代参数划分为多个数据块，作为单个任务提交给进程池的工
      作进程。每个数据块的近似大小可使用chunksize参数指定。

***** map_async(func,iterable[,chunksize[,callback[,error_callback]]])
      Pool.map()的并行版本。

      chunksize用法与Pool.map()用法相同。

      callback和error_callback用法与Pool.map()用法相同。
***** imap(func,iterable[,chunksize])
      map()的惰性求值版本，即返回值不为链表，而为iterator。且具有
      next()方法。

      chunksize：用法与map()用法相同，对于元素非常多的参数，较之于默认
      值1，将此值设置为大型数字可大大提高计算速度。

      对于imap，如果chunksize值为1，则在返回结果上调用next()方法时，可
      使用next(timeout)，当在指定时长中不能获取返回值时，抛出
      multiprocessing.TimeoutError错误。

***** imap_unordered(func,iterable[,chunksize])
      类似map()，但结果序列中元素的顺序不定。只有单个工作进程操作时，
      才能保证结果序列中元素顺序。

***** starmap(func,iterable[,chunksize])
      类似于map()，不过iterable参数中的元素需为可迭代对象，分解作为
      func的参数。

      如果iterable为[(1,2), (3,4)]，则结果为[func(1,2), func(3,4)]

***** starmap_async(func,iterable[,chunksize[,error_back]])
      starmap()和map_async()的结合体。

***** close()
      防止再有任务提交给进程池对象。一旦所有任务完成，所有工作进程都退
      出。

***** terminate()
      无条件终止所有工作进程，不管是否有完成任务。当进程池对象被垃圾回
      收时，会自动调用terminate()方法。

***** join()
      等待所有工作进程退出。需在close()或terminate()方法前调用join()方
      法。

**** class multiprocessing.pool. *AsyncResult*
     Pool.apply_async()和Pool.map_async()返回结果类型。

***** get([timeout])
      返回已经得到的结果。

      timeout：如果不为None，如果指定秒数内不能获取结果，则抛出
      multiprocessing.TimeError错误。

      如果使用apply_async()和map_async()计算时抛出错误，则会被get()重
      新抛出。

***** wait([timeout])
      等待直到结果可获取，或timeout指定时长。

***** ready()
      返回表示计算是否执行完成的布尔值。

***** successful()
      返回表示计算是否执行完成，且没有错误抛出的布尔值。如果还不能获取
      结果，会抛出AssertionError错误。

**** 用法举例

     #+BEGIN_EXAMPLE python
       from multiprocessing import Pool
       import time

       def f(i): return i * i

       if __name__ == '__main__':
           with Pool(processes=4) as pool:  # 开启4个工作单位进程
               result = pool.apply_async(f, (10,))  # 在单个进程中异步计算f(10)，
               print(result.get(timeout=1))        # 除非计算机“相当慢”，打印100

               print(pool.map(f, range(10)))  # 打印[0, 1, 4,..., 81]

               it = pool.imap(f, range(10))
               print(next(it))         # 打印0
               print(next(it))         # 打印1
               print(it.next(timeout=1))  # 除非计算机“相当慢”，打印4

               result = pool.apply_async(time.sleep, (10,))
               print(result.get(timeout=1))  # 抛出multiprocessing.TimeError错误
     #+END_EXAMPLE

*** 监听器和客户端
    一般情况下，进程间的消息传递都是使用队列，或Pipe()返回的Connection
    对象。

    但是，multiprocessing.connection模块提供了额外的工具，使消息传递更
    灵活。提供高阶消息传递API，处理套接字，或Windows的命名管道。还支持
    使用hmac模块进行“摘要式身份验证（digest authentication）”，以及同
    时轮询多个连接对象。

**** multiprocessing.connection. *deliver_challenger* (connection,authkey)
     向连接的另一端发送随机生成的消息，等待回应。

     如果回应使用authkey满足摘要消息，则向连接另一端发送欢迎消息。否则
     抛出AuthenticationError。

**** multiprocessing.connection. *answer_challenge* (connection,authkey)
     接收消息，使用authey计算摘要消息，然后将摘要发送回去。

     如果不能接收到欢迎消息，抛出AuthenticationError错误。

**** multiprocessing.connection. *Client* (address[,family[,authenticate[,authkey]]])
     尝试与使用address作为地址监听进程连接，返回连接对象。

     连接的类型由family参数决定，不过一般可省略，因为通常通过address参
     数自动推断。

     authenticate：如果为True，或authkey为bytes对象，则使用摘要式消息
     验证。如果指定authkey，则用于消息验证，如果authkey为None，则使用
     current_process().authkey。如果验证失败，则抛出
     AuthenticationError错误。

**** multiprocessing.connection. *Listener* ([address[,family[,backlog[,authenticate[,authkey]]]]])
     对限制性套接字（bound socket）或Windows命名管道的包装，用于“监听”连接。

     address：监听对象限制性套接字或命名管道的地址。

     #+BEGIN_QUOTE
     *注意* ： 如果地址为“0.0.0.0”，则在Windows中作为连接终端，需使用
     “127.0.0.1”。
     #+END_QUOTE

     family：指定套接字或命名管道的类型。可为字符串'AF_INET'（TCP套接
     字）、'AF_UNIX'（Unix 域套接字）或'AF_PIPE'（Windows命名管道）。
     其中只有第一个能保证始终有效。如果为None，则通过address值推断。
     如果address为None，则使用默认值。默认值为最快的类型。

     注意，如果family为'AF_UNIX'，且address参数为None，则套接字会使用
     tempfile.mkstemp()创建在私有文件夹中。

     如果监听对象使用套接字，则当套接字绑定后，socket.listen()中
     使用1作为backlog参数值。

     如果authenticate为True（默认为False）或authkey不为None，则使用摘
     要式验证。

     authykey：如果指定，须为bytes对象，用于摘要验证；如果为None，且
     authenticate参数为True，则使用current_process().authkey验证；如
     果authkey为None，且authenticate为False，则不验证。如果验证失败，
     抛出AuthenticationError错误。

     支持with上下文管理器，__enter__()方法返回监听对象，__exit__()方法
     调用close()方法。

***** accept()
      接受在监听对象限制性套接字或命名管道上的连接，返回Connection对象。
      如果尝试验证且失败，则抛出AuthenticationError错误。

***** close()
      关闭监听对象上的限制性套接字或命名管道。当监听对象被垃圾回收时自
      动调用，但建议显示调用。

***** address
      监听对象使用的地址。

***** last_accepted
      上次接受连接的来源地址，如果没有连接则返回None。

**** multiprocessing.connection. *wait* (object_list,timeout=None)
     等待直到object_list中至少有一个对象准备好。返回object_list中已经
     准备好的对象组成的链表。

     timeout：直到最多等待秒数的浮点数。如果为None，则无限期等待至有对
     象准备好为止。负数相当于0。

     不管是Unix，还是Windows中，object_list链表中的对象可为：

     - 可读的Connection对象；
     - 已连接，且可读的socket.socket对象；或
     - Process对象的sentinel属性。

     如果可从连接对象或套接字对象，或另一端已经关闭表示已准备好。

     *Unix* ：wait(object_list, timeout)基本上等价于
     select.select(object_list, [], [], timeout)。不同之处为，如果
     select.select()被信号中断，抛出错误码为EINTER的OSError错误，而
     wait()则不会。

     *Windows* ：object_list中的元素须为可写的整数句柄，或有fileno()方
     法的对象（根据Win32的WiatForMultipleObject()的定义），返回套接字
     句柄或管道句柄。（注意，套接字和管道句柄都为不可写句柄。）

**** 举例

     创建使用"secret password"验证的监听对象，等待客户端连接并向其发送
     数据：

     #+BEGIN_EXAMPLE python
       from multiprocessing.connection import Listener

       from array import array

       address = ('localhost', 9999) # family被推断为AF_INET

       with Listener(address, authkey=b'secret password') as listener:
           with listener.accept() as conn:
               print('connnection accept from', listener.last_accepted)
               conn.send([2.25, None, 'junk', float])
               conn.send_bytes(b'hello')
               conn.send_bytes(array('i', [42, 1729]))
     #+END_EXAMPLE

     创建客户端，并从服务器接收数据：

     #+BEGIN_EXAMPLE python
       from multiprocessing.connection import Client
       from array import array

       address = ('localhost', 9999)

       with Client(address, authkey=b'password') as conn:
            print(conn.recv())
            print(conn.recv_bytes())
            arr = array('i', [0, 0, 0, 0, 0])
            print(conn.recv_bytes_into(arr))
            print(arr)
     #+END_EXAMPLE

     使用wait()同时等待多个进程的消息：

     #+BEGIN_EXAMPLE python
       from multiprocessing import Process, Pipe, current_process
       from multiprocessing.connection import wait

       def foo(w):
           for i in range(10):
               w.send((i, current_process().name))
           w.close()

       if __name__ == '__main__':
           readers = []
           for i in range(4):
               r, w = Pipe(duplex=False)
               readers.append(r)
               p = Process(target=foo, args=(w,))
               p.start()

           while readers:
               for r in wait(readers):
                   try:
                       msg = r.recv()
                   except EOFError:
                       readers.remove(r)
                   else:
                       print(msg)
     #+END_EXAMPLE

**** 地址格式

     - AF_INET：以元组(hostname, port)表示的地址，其中hostname为字符串，
       port为正整数。

     - AF_UNIX：以字符串表示文件系统中的文件名。

     - AF_PIPE：字符串形式，r'\\.\pipe\PipeName'。如果使用Client()连接
       远程主机上名为ServerName的命名管道，须为
       r'\\ServerName\pipe\PipeName'。

     注意：任何以双反斜杠开头的地址都被识别为AF_PIPE，而非AF_UNIX。

*** 授权键（Authentication keys）
    当使用Connection.recv时，任何接收数据都自动unpickled。但是，
    unpickle数据如果来自不受信任的源，则存在安全隐患。所以Listener和
    Client()使用hmac模块进行摘要式验证。

    授权键可被认为是密码：一旦连接建立，两端就进行校验。（如果两端使用
    相同授权键，不会在连接中传递授权键。）

    如果指定需要验证，且没有提供授权键，则使用
    current_process().authkey。该值会自动被当前进程中创建的进程基础。
    即默认情况下，所有多进程程序中的进程搜会使用相同授权键，并可用于连
    接验证。

    生成授权键最合适的方法是使用os.urandom()

*** Logging ？？？
    虽然支持一些logging。但是logging包不适用进程共享互斥锁，所以可能
    （依赖于句柄类型）从不同进程中获取的消息会出现混乱。

**** multiprocessing. *get_logger* ()
     返回multiprocessing模块使用的logger。如果需要，新建。

     第一次创建logger时，其层级为logging.NOTSET，且无默认处理函数。被
     发送到此logger的消息不会默认传播到根logger。

     Windows中，子进程只会继承父进程logger的层级——对logger的任何定制不
     会被继承。

**** multiprocessing. *log_to_stderr* ()
     执行get_logger，且返回get_logger创建的logger，使
     用'[%(levelname)s/%(processName)s] %(message)s'添加将输出内容到
     sys.stderr的处理函数。

     如：

     #+BEGIN_EXAMPLE python
       >>> import multiprocessing, logging
       >>> logger = multiprocessing.log_to_stderr()
       >>> logger.setLevel(logging.INFO)
       >>> logger.warning('doomed')
       [WARNING/MainProcess] doomed
       >>> m = multiprocessing.Manager()
       [INFO/SyncManager-...] child process calling self.run()
       [INFO/SyncManager-...] created temp directory /.../pymp-...
       [INFO/SyncManager-...] manager serving at '/.../pymp-.../listener-...'
       >>> del m
       [INFO/MainProcess] sending shutdown message to manager
       [INFO/SyncManager-...] process shutting down
       [INFO/SyncManager-...] process exiting with exitcode 0
     #+END_EXAMPLE

**** multiprocessing. *get_logger* ()
**** multiprocessing. *log_to_stderr* ()
*** multiprocessing.dummy模块
    将threading模块包装为multiprocessing模块API。

** 编程手册
   下面为使用multiprocessing模块时需遵守的原则。

*** 针对所有启动方法

    下面的直到适用于所有启动方法。

    - 避免共享状态

      须尽量避免在进程间传递大龄数据。

      尽可能使用队列和管道实现进程间交流，而避免使用底层的同步原始。

    - picklablity

      保证代理对象方法的参数为picklable。

    - 代理对象的线程安全

      除非使用互斥锁保护，不得使用来自多于一个线程的代理对象。

      不同进程使用相同代理对象完全无害。

    - join僵尸进程

      在Unix中，当一个进程执行结束，但没被join，会成为僵尸进程。虽然不
      会有很多，因为当一个新进场创建时，或调用active_children()时都会
      自动join所有，且调用执行完毕进程的is_alive()方法时都会join该进程；
      但是，最好显示join所有开启的进程。

    - 继承优于pickle/unpickle

      当使用spawn和forkserver方法启动进程时，multiprocessing模块中的许
      多数据类型都需为picklable，以便子进程可使用。但是，应避免使用管
      道或队列发送共享数据到子进程。应合理组织程序，让子进程需使用的共
      享数据继承自祖先进程。

    - 避免手动终止子进程

      使用Process.terminate方法停止某个进程时，可能造成当前被使用的共
      享资源（如互斥锁、信号量、管道和队列）被破坏，或不能被其他进程使
      用。
      
      所有，只有当不再需要任何共享资源时才使用Process.terminate。
      
    - 对使用队列的进程合理使用join操作

      要记住，如果一个子进程已经向队列中添加元素，会等待直达所有缓存元
      素被“供给”线程供给底层管道后才能退出。（子进程可调用
      Queue.cancel_join_thread方法避免此行为，但不建议使用。）

      意味着，当使用队列时，需保证在join进程前需将所有添加到队列中的元
      素移除。否则，不能保证向队列添加元素的进程会终止。还需记住，所有
      非后台进程都会自动join。

      下面为一个会锁死的例子：

      #+BEGIN_EXAMPLE python
      import multiprocessing as mp
      import time

      def f(q):
          q.put('x' * 1000000)

      if __name__ == '__main__':
          queue = mp.Queue()
          p = mp.Process(target=f, args=(queue,))
          p.start()
          p.join()             # 锁死
          obj = queue.get()
      #+END_EXAMPLE

      解决方法是将倒数两行交换位置。

    - 显示传递资源给子进程

      Unix中默认使用fork作为子进程创建方法，可将父进程的共享资源作为全
      局共享资源。但是，最好的做法还是将共享对象作为子进程构造器的参数
      传递。

      除可与Windows的spawn启动方法和其他启动方法兼容外，还可保证只要子
      进程存活，就不会被父进程垃圾回收。

      所以：

      #+BEGIN_EXAMPLE python
      from multiprocessing import Process, Lock

      def f():
          ... do something using "lock" ...

      if __name__ == '__main__':
         lock = Lock()
         for i in range(10):
             Process(target=f).start()
      #+END_EXAMPLE

    可修改为：

    #+BEGIN_EXAMPLE python
      from multiprocessing import Process, Lock

      def f(l):
          ... do something using "l" ...

      if __name__ == '__main__':
         lock = Lock()
         for i in range(10):
             Process(target=f, args=(lock, )).start()
    #+END_EXAMPLE

    - 不要将sys.stdin替换为“类文件对象” ？？？

      起先，在multiprocessing.Process._bootstrap()中无调价调用：

      #+BEGIN_EXAMPLE python
        os.close(sys.stdin.fileno())
      #+END_EXAMPLE

      但会在“进程中的进程”存在问题，最后修改为：

      #+BEGIN_EXAMPLE python
        sys.stdin.close()
        sys.stdin = open(os.open(os.devnull, os.O_RDONLY), closefd=False)
      #+END_EXAMPLE

      解决了进程间碰撞，造成无效文件描述符的问题，但也引入了将
      sys.stdin()替换为无输出缓存类文件对象的危险。如果多个进程对这个
      类文件对象调用close()方法，会造成相同的数据多次flushed到该类文件
      对象。

      如果要编写类文件对象来实现缓存，可通过保存PID的方法来确保fork安
      全，当PID改变是忽略缓存。比如：

      #+BEGIN_EXAMPLE python
        @property
        def cache(self):
            pid = os.getpid()
            if pid != self._pid:
               self._pid = pid
               sefl._cache = []
            return sefl._cache
      #+END_EXAMPLE

*** 针对spawn和forkserver启动方法
    下面为除fork外，另外两个启动方法的限制。

    - 更多picklablity

      保证所有Process.__init__()的参数都为picklable。同样，如果声明
      Process的子类，要保证调用Process.start()时其实例为picklable。

    - 全局变量

      要记住，如果尝试在子进程中访问全局变量，访问时的全局变量值可能与
      调用Process.start时的值不同。

      然而，模块乘积的常量不会引发相同问题。

    - 安全导入主模块

      保证主模块可被新的Python解释器导入，不引发如开启新进程之类的副作
      用。

      比如，使用spawn或forkserver启动方法允许下面代码会引发
      RuntimeError：

      #+BEGIN_EXAMPLE python
        from multiprocessing import Process

        def foo(): print('hello')

        p = Process(target=foo)
        p.start()
      #+END_EXAMPLE

      应将“接入点”保护在if __name__ == '__main__'语句中：

      #+BEGIN_EXAMPLE python
        from multiprocessing import Process, freeze_support, set_start_method

        def foo():print('hello')

        if __name__ == '__main__':
           freeze_support()
           set_start_method('spawn')
           p = Process(target=foo)
           p.start()
      #+END_EXAMPLE

      可保证新开启的Python解释器安全导入模块，并允许模块的foo()函数。

      如果在主模块中创建进程池或管理器也会有相同限制。

** 举例
*** 如何创建和定制管理器和代理对象：

    #+BEGIN_EXAMPLE python
      from multiprocessing import freeze_support
      from multiprocessing.managers import BaseManager, BaseProxy
      import operator

      #
      #
      #

      class Foo:
          def f(self):
              print('调用了Foo.f()')

          def g(self):
              print('调用了Foo.g()')

          def _h(self):
              print('调用了Foo._h()')

      # 一个简单的生成器函数

      def baz():
          for i in range(10):
              yield i * i

      # 生成器对象的代理类型

      class GeneratorProxy(BaseProxy):
          _exposed_ = ['__next__']

          def __iter__(self):
              return self

          def __next__(self):
              return self._callmethod('__next__')

      # 返回operator模块的函数
      def get_operator_module():
          return operator


      #
      #
      #

      class MyManager(BaseManager):
          pass


      # 注册类Foo；使f()和g()可通过代理访问
      MyManager.register('Foo1', Foo)

      # 注册类Foo；使g()和_h()可通过代理访问
      MyManager.register('Foo2', Foo, exposed=('g', '_h'))

      # 注册生成器函数baz；并使用GeneratorProxy创建代理
      MyManager.register('baz', baz, proxytype=GeneratorProxy)

      # 注册get_operator_module()函数；让公用函数可通过代理访问
      MyManager.register('operator', get_operator_module)

      #
      #
      #


      def test():
          manager = MyManager()
          manager.start()

          print('-' * 20)
          f1 = manager.Foo1()
          f1.f()
          f1.g()

          assert not hasattr(f1, '_h')
          assert sorted(f1._exposed_) == sorted(['f', 'g'])

          print('-' * 20)
          f2 = manager.Foo2()
          f2.g()
          f2._h()
          assert not hasattr(f2, 'f')
          assert sorted(f2._exposed_) == sorted(['g', '_h'])

          print('-' * 20)
          it = manager.baz()
          for i in it:
              print('<{}>'.format(i), end=' ')
          print()

          print('-' * 20)

          op = manager.operator()
          print('op.add(23, 45) = ', op.add(23, 45))
          print('op.pow(2, 94) = ', op.pow(2, 94))
          print('op._exposed_ = ', op._exposed_)
      #
      #
      #

      if __name__ == '__main__':
          freeze_support()
          test()
    #+END_EXAMPLE

*** 使用进程池Pool

    #+BEGIN_EXAMPLE python
      import multiprocessing
      import time
      import random
      import sys

      #
      # 测试代码使用的函数
      #

      def caculate(func, args):
          result = func(*args)
          return '{}得出 {}{} = {}'.format(
              multiprocessing.current_process().name,
              func.__name__, args, result
          )

      def caculatestar(args):
          return caculate(*args)

      def fooz():
          time.sleep(1 * random.random())

      def mul(a, b):
          fooz()
          return a * b

      def plus(a, b):
          fooz()
          return a + b

      def f(x):
          return 1.0 / (x - 5.0)

      def pow3(x):
          return x ** 3


      def noop(x):
          pass

      #
      # 测试代码
      #


      def test():
          PROCESSES = 4
          print('进程池中使用{}个工作进程'.format(PROCESSES))
          with multiprocessing.Pool(PROCESSES) as pool:
              # 开始测试

              TASKS = [(mul, (i, 7)) for i in range(10)] +\
                      [(plus, (i, 8)) for i in range(10)]

              results = [pool.apply_async(caculate, t) for t in TASKS]
              imap_it = pool.imap(caculatestar, TASKS)
              imap_unordered_it = pool.imap_unordered(caculatestar, TASKS)

              print('pool.apply_async()得出的排序结果为：')
              for r in results:
                  print('\t', r.get())
              print()

              print('pool.imap()得出的排序结果为：')
              for x in imap_it:
                  print('\t', x)
              print()

              print('pool.imap_unordered()得出的非排序结果为：')
              for x in imap_unordered_it:
                  print('\t', x)
              print()

              #
              # 测试错误处理
              #

              print('测试错误处理')

              try:
                  print(pool.apply(f, (5,)))
              except ZeroDivisionError:
                  print('\t运行pool.apply时抛出ZeroDivisionError错误')
              else:
                  raise AssertionError('希望得到ZeroDivisionError')

              try:
                  print(pool.map(f, list(range(10))))
              except ZeroDivisionError:
                  print('\t运行pool.map时抛出ZeroDivisionError错误')
              else:
                  raise AssertionError('希望得到zerodivisionerror')

              try:
                  print(list(pool.imap(f, range(10))))
              except ZeroDivisionError:
                  print('\t运行pool.map时抛出ZeroDivisionError错误')
              else:
                  raise AssertionError('希望得到zerodivisionerror')

              it = pool.imap(f, list(range(10)))
              for i in range(10):
                  try:
                      x = it.next()
                  except ZeroDivisionError:
                      if i == 5:
                          pass
                  except StopIteration:
                      break
                  else:
                      if i == 5:
                          raise AssertionError('希望得到ZerodivisionError')

              assert i == 9
              print('\t成功从IMapIterator.next()方法中抛出一次ZerodivisionError')
              print()

              #
              # 测试超时
              #

              print('测试ApplyResult.get()超时：', end=' ')
              res = pool.apply_async(caculate, TASKS[0])
              while 1:
                  sys.stdout.flush()
                  try:
                      sys.stdout.write('\n\t{}'.format(res.get(0.02)))
                  except multiprocessing.TimeoutError:
                      sys.stdout.write('.')
                  else:
                      break
              print()
              print()

              print('测试IMapIterator.next()超时：', end=' ')
              it = pool.imap(caculatestar, TASKS)
              while 1:
                  sys.stdout.flush()
                  try:
                      sys.stdout.write('\n\t{}'.format(it.next(0.02)))
                  except StopIteration:
                      break
                  except multiprocessing.TimeoutError:
                      sys.stdout.write('.')
              print()
              print()


      if __name__ == '__main__':
          multiprocessing.freeze_support()
          test()
    #+END_EXAMPLE

*** 如何利用队列向多个进程提交任务，并收集处理结果：

    #+BEGIN_SRC python :eval never
      import random
      import time
      from multiprocessing import Process, Queue, current_process, freeze_support

      #
      # 工作线程执行函数
      #

      def worker(input, output):
          for func, args in iter(input.get, 'STOP'): # 一直迭代，当队列中不存在元素时阻塞，直到获取“STOP”
              result = calculate(func, args)
              output.put(result)

      #
      # 用于计算结果的函数
      #

      def calculate(func, args):
          result = func(*args)
          return '{} 计算得出 {}{} = {}'.format(current_process().name,
                                            func.__name__, args, result)

      #
      # 被任务索引的函数
      #

      def mul(a, b):
          time.sleep(0.5 * random.random())
          return a * b

      def plus(a, b):
          time.sleep(0.5 * random.random())
          return a + b

      #
      #
      #

      def test():
          NUMBER_OF_PROCESSES = 4
          TASK1 = [(mul, (i, 7)) for i in range(20)]
          TASK2 = [(plus, (i, 8))for i in range(10)]

          # 创建队列
          task_queue = Queue()
          done_queue = Queue()

          # 提交任务
          for task in TASK1:
              task_queue.put(task)

          # 开启工作进程
          for i in range(NUMBER_OF_PROCESSES):
              Process(target=worker, args=(task_queue, done_queue)).start()

          # 获取并打印结果：
          print('非顺序结果：')
          for i in range(len(TASK1)):
              print('\t', done_queue.get())

          # 使用put()日安家更多任务
          for task in TASK2:
              task_queue.put(task)

          # 显示更多结果
          print('-' * 10)
          for i in range(len(TASK2)):
              print('\t', done_queue.get())

          # 告知所有子进程停止工作
          for i in range(NUMBER_OF_PROCESSES):
              task_queue.put('STOP')

      if __name__ == '__main__':
          freeze_support()
          test()
    #+END_SRC

* concurrent包
  目前为止，此包中只有一个模块：

  + concurrent.futures - 发起并发任务

* concurrent.futures - 发起并发任务
  concurrent.futures模块提供异步执行调用的高阶接口。

  异步执行可利用ThreadPoolExecutor通过线程实现，也可利用
  ProcessPoolExecutor通过进程实现。两个实现都使用相同接口，用抽象类
  Executor实现。

** Executor对象
*** class concurrent.futures. *Executor*

    提供异步执行方法的抽象类。不得直接使用，而是通过具体子类使用。

**** submit(fn,*args,**kwargs)
     调度可调用对象fn，执行fn(*args, **kwargs)，返回表示执行过程的
     Future对象。

     #+BEGIN_EXAMPLE python
       from concurrent.futures import ThreadPoolExecutor

       with ThreadPoolExecutor(max_workers=1) as executor:
           future = executor.submit(pow, 323, 12345)
           print(future.result())
     #+END_EXAMPLE

**** map(func,*iterables,timeout=None,chunksize=1)
     等价于map(func, *iterables)，不过会异步执行func，且可能将func分配
     为多次并发调用。

     返回迭代器对象。如果func调用有错误抛出，则当访问迭代器中对应元素
     时重新抛出。

     timeout：从调用Executor.map()算起，如果在返回迭代器上调
     用__next__()方法在timeout时长内不能获取，则抛出
     concurrent.futures.TimeoutError错误。如果不指定或为None，则可无限
     期等待结果。

     chunksize：指定对数据分段近似大小。当使用ProcessPoolExecutor时，
     会将待处理数据分段，默认值为1。对应大型序列，将chunksize设置为较
     大数值可大大提高效率。对于ThreadPoolExecutor，此参数没有任何意义。

**** shutdown(wait=True)
     向“执行器（executor）”发送信号，指定当当前所有futures完成后释放所
     有资源。在Executor.submit()和Executor.map()前调用会抛出
     RuntimeError。

     wait：如果为True（默认），直到所有未完成的futures完成后，且所有资
     源释放后才返回；如果为False，则立即返回，等待所有为执行完futures
     执行完成后释放所有资源；不管为False还是True，整个Python程序都会在
     所有为完成futures执行完成后才退出。

     可使用with上下文管理器避免使用此方法，__exit__会调用此方法，且
     wait参数默认为True：

     #+BEGIN_EXAMPLE python
       import shutil
       with ThreadPoolExecutor(max_workers=4) as e:
            e.submit(shutil.copy, 'src1.txt', 'dest1.txt')
            e.submit(shutil.copy, 'src2.txt', 'dest2.txt')
            e.submit(shutil.copy, 'src3.txt', 'dest3.txt')
     #+END_EXAMPLE

** ThreadPoolExecutor
   ThreadPoolExecutor为Executor的子类，使用线程池实现异步调用。

   当与Future对象关联的可调用对象等待另一个Future返回结果时，会造成锁
   死，如：

   #+BEGIN_EXAMPLE python
     import time
     from concurrent.futures import ThreadPoolExecutor

     def wait_on_b():
         time.sleep(5)
         print(b.result())           # b永远不会完成，因为正在等待a。
         return 5

     def wait_on_a():
         time.sleep(5)
         print(a.result())           # a永远不会完成，因为正在等待b
         return 6

     executor = ThreadPoolExecutor(max_workers=2)
     a = executor.submit(wait_on_a)
     b = executor.submit(wait_on_b)
   #+END_EXAMPLE

   和：

   #+BEGIN_EXAMPLE python
     def wait_on_future():
         f = executor.submit(pow, 5, 2)
         # 永远不会完成，因为这是唯一的工作线程，且
         # 正则执行本函数
         print(f.result)

     executor = ThreadPoolExecutor(max_workers=1)
     executor.submit(wait_on_future)
   #+END_EXAMPLE

*** class concurrent.futures. *ThreadPoolExecutor* (max_workers=None)
    Executor子类，使用最多max_workers个数的线程组成的线程池实现异步调
    用。

    max_Workers：如果不指定或为None（默认），则默认个数为计算机处理器
    个数乘以5，假设ThreadPoolExecutor被用于I/O密集计算，而非CPU密集计
    算，工作线程数量应该比ProcessPoolExecutor的工作进程数量高。

*** ThreadPoolExecutor举例

    #+BEGIN_EXAMPLE python
      import concurrent.futures
      import urllib.request

      URLS = [
          'http://www.baidu.com',
          'http://www.bing.com',
          'http://www.qq.com',
          'http://www.google.com',
      ]

      # 访问单个页面，报告页面内容
      def load_url(url, timeout):
          print('开始请求页面：', url)
          with urllib.request.urlopen(url, timeout=timeout) as conn:
              return conn.read()

      # 使用with上下文管理器，保证线程资源被释放
      with concurrent.futures.ThreadPoolExecutor(max_workers=len(URLS)) as executor:
          # 加载操作，标记各个future及其URL
          future_to_url = {executor.submit(load_url, url, 2): url for url in URLS}
          for future in concurrent.futures.as_completed(future_to_url.keys()):
              url = future_to_url[future]
              try:
                  data = future.result()
              except Exception as exc:
                  print('{!r}抛出错误：{}'.format(url, exc))
              else:
                  print('{!r}页面读取了{}个字节'.format(url, len(data)))
    #+END_EXAMPLE

** ProcessPoolExecuter

*** class concurrent.futures. *ProcessPoolExecutor* (max_workers=None)
*** ProcessPoolExecutor举例
** Future对象
*** class concurrent.futures. *Future*
**** 普通方法
***** cancel()
***** cancelled()
***** running()
***** done()
***** result(timeout=None)
***** exception(timeout=None)
***** add_done_callback(fn)
**** 用于单元测试和Executor实现的方法
***** set_running_or_notify_cancel()
***** set_result(result)
***** set_exception(exception)

** 模块函数

*** class concurrent.futures. *wait* (fs,timeout=None,return_when=ALL_COMPLETED)

*** class concurrent.futures. *as_completed* (fs,timeout=None)

** Exception类

*** exception concurrent.futures. *CancelledError*

*** exception concurrent.futures. *TimeoutError*

*** exception concurrent.futures. *BrokenProcessPool*

* subprocess
  subprocess模块用于产生新进程，连接自己的输出/输入/错误流管道，并获取
  返回码。此模块旨在替换几个旧模块和函数：

  #+BEGIN_EXAMPLE python
    os.system
    os.spawn*
  #+END_EXAMPLE

** 使用subprocess模块
   如需激活子进程，建议尽量使用run()函数；如需更高阶定制，可直接使用底
   层的Popen接口。

   run()函数在Python3.5时才新增，如需向后兼容，参考后面的“旧高阶API”。

*** subprocess. *run* (args,*,stdin=None,input=None,stderr=None,shell=False,timeout=None,check=False)
    执行ARGS指定的命令，等待执行结束，返回CompleteProcess实例。

    参数使用方法参考后面的“常用参数”。用法基本上与Popen构造器的参数用
    法相同，timeout、input和check除外。实际上，本函数所有参数都传递给
    Popen接口。

    默认情况下，不会捕获标准输出和标准错误流。如需捕获，传递PIPI作为
    STDOUT/STDERR参数值。

    TIMEOUT参数实际上传递给Popen.communicate()函数。如果超时，子进程被
    干掉，然后等待。子进程被终止后，TimeExpired错误会重新抛出。

    INPUT参数也会传递给Popen.communicate()函数，作为子进程的标准输入流。
    如果使用，须为比特序列，或参数UNIVERSAL_NEWLINES=True的情况下可为
    字符串。使用后，内部的Popen对象会自动将STDIN参数设置为PIPE，就可能
    不使用本函数的STDIN参数。

    如果CHECK参数为true，且子进程返回码（exit code）不为0，则抛出
    CalledProcessError错误。如果捕获stdout和stderr，从抛出Exception的
    属性可获取本函数参数，以及退出码等信息。

    举例：

    #+BEGIN_EXAMPLE sh
      >>> subprocess.run(['ls', '-l']) # 不捕获输出，只是打印
      total 0
      -rw-r--r-- 1 claudio claudio 0 Jun  3 15:53 text.txt
      CompletedProcess(args=['ls', '-l'], returncode=0)
      >>> subprocess.run('exit 1', shell=True)
      CompletedProcess(args='exit 1', returncode=1)
      >>> subprocess.run('exit 1', shell=True, check=True)
      Traceback (most recent call last):
        File "<stdin>", line 1, in <module>
        File "/usr/lib/python3.5/subprocess.py", line 398, in run
        output=stdout, stderr=stderr)
      >>> subprocess.run(['ls', '-l', '/dev/null'], stdout=subprocess.PIPE) # 捕获输出
      CompletedProcess(args=['ls', '-l', '/dev/null'], returncode=0, stdout=b'crw-rw-rw- 1 root root 1, 3 Jun  2 10:27 /dev/null\n')
    #+END_EXAMPLE

*** class subprocess. *CompletedProcess*
    run()函数返回值，表示该子进程已经执行完成。有下面属性。

**** args
     传递给run()函数执行的命令，可能是字符串也可能为链表。

**** returncode
     子进程返回码，0表示子进程成功执行完成。

     POSIX中，如果为数值-N，表示子进程被信号N终止。

**** stdout
     从子进程标准输出流捕获的内容，一般为bytes对象，如果run()使用参数
     universal_newlines=True参数，则为字符串。

**** stderr
     从子进程标准错误流捕获的内容，一般为bytes对象，如果run()使用参数
     universal_newlines=True参数，则为字符串。
**** check_returncode()
     如果returncode不为0，抛出CalledProcessError错误，否则返回None。

*** subprocess. *DEVNULL*
    可用于Popen的STDIN、STDOUT和STDERR参数的特殊值，表示使用特殊文件
    os.devnull。

*** subprocess. *PIPE*
    可用于Popen的STDIN、STDOUT和STDERR参数的特殊值，表示对应标准流会打
    开一个管道。与Popen.communicate()一起使用更有用。

*** subprocess. *STDOUT*
    用于Popen的stderr参数的特殊值，当stderr=subprocess.STDOUT时，可将
    标准错误流重定向到标准输出流。

*** exception subprocess. *SubprocessError*
    本模块所有exception的基础类。

*** exception subprocess. *TimeoutExpired*
    SubprocessError的子类，当等待子进程超时时抛出。有如需属性：

    #+BEGIN_SRC python :session
      import subprocess
      import sys
      DECODING = sys.getdefaultencoding()
      try:
          subprocess.run(['ping', 'bing.com'], stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE, timeout=2)
      except subprocess.TimeoutExpired as time_why:
          print('cmd：', time_why.cmd)
          print('timeout：', time_why.timeout)
          print('output：', time_why.output.decode(DECODING))
          print('stdout：', time_why.stdout.decode(DECODING))
          print('stderr：', time_why.stderr.decode(DECODING))
    #+END_SRC

    #+RESULTS:
    #+BEGIN_SRC org
    - >>> >>> >>> ... ... ... ... ... ... ... ... ... cmd： ['ping', 'bing.com']
    - timeout： 2
    - output： PING bing.com (204.79.197.200) 56(84) bytes of data.
    - 64 bytes from bing.com (204.79.197.200): icmp_seq=2 ttl=116 time=49.1 ms
    - stdout： PING bing.com (204.79.197.200) 56(84) bytes of data.
    - 64 bytes from bing.com (204.79.197.200): icmp_seq=2 ttl=116 time=49.1 ms
    - stderr：
    #+END_SRC

**** cmd
     激活子进程的命令。
**** timeout
     超时时长秒数。
**** output
     子进程输出（如果被run()或check_output()捕获），否则为None。
**** stdout
     output的别名，为保证与stderr语义一致。
**** stderr
     子进程标准错误流（如果被run()捕获），否则为None。

*** exception subprocess. *CalledProcessError*
    SubprocessError子类，当进程被check_all()或check_ooutput()执行返回
    非0返回码时抛出。

    #+BEGIN_SRC python :session
      import subprocess

      try:
          proc = subprocess.run(
              ['ls', 'not-exists'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
      except subprocess.CalledProcessError as why:
          print('returncode：', why.returncode)
          print('cmd：', why.cmd)
          print('output：', why.output)
          print('stdout：', why.stdout)
          print('stderr：', why.stderr)
    #+END_SRC

    #+RESULTS:
    #+BEGIN_SRC org
    - >>> ... ... ... ... ... ... ... ... ... returncode： 2
    - cmd： ['ls', 'not-exists']
    - output： b''
    - stdout： b''
    - stderr： b"ls: cannot access 'not-exists': No such file or directory\n"
    #+END_SRC

**** returncode
     子进程返回码。如果子进程因信号退出，则为表示信号的负数。
**** cmd
     激活子进程的命令。

**** output
     子进程输出（如果被run()或check_output()捕获），否则为None。
**** stdout
     output的别名，为保证与stderr语义一致。
**** stderr
     子进程标准错误流（如果被run()捕获），否则为None。

*** 常用参数
    为支持不同的使用场景，Popen构造器和run()函数使用大量可选参数。大多
    数情况下，许多参数可省略使用默认值，常用的参数如下：

**** args
     执行命令，可为字符，也可为命令和参数组成的序列（如链表）。首选序
     列，这样可方便处理命令参数的转义和添加引号（如允许命令参数为有空
     格的字符串）。如果为单个字符串，要么SHELL参数需为True，要么执行命
     令不得带任何参数。

**** stdin、stdout和stderr
     表示命令执行的标准输入流、输出流和错误流定向方式。可选值为
     subprocess.PIPE、subprocess.DEVNULL、已存在的文件描述符（正数）、
     已存在的文件对象或None。PIPE表示会为子进程打开一个管道。DEVNULL表
     示使用os.devnull文件。默认为None，表示不对标准流进行任何重定向。
     子进程文件处理继承于父进程。此外，STDERR可设置为subprocess.STDOUT，
     将标准错误流重定向到标准输出流。

**** universal_newlines
     如果UNIVERSAL_NEWLINES为False，则stdin、stdout和stderr都使用二进
     制模式，不会自动转换换行符。

     如果UNIVERSAL_NEWLINES为True，则文件对象以文本模式打开，使用
     locale.getpreferredecodiong(False)编码和解码。对应stdin，换行符
     "\n"自动转换为os.linesep；对于stdout，所有输入的换行符都转换为
     "\n"。

     #+BEGIN_QUOTE
     *注意* ：Popen.communicate()不会自动更新Popen.stdin、Popen.stdout
     和Popen.stderr不会的换行符属性。
     #+END_QUOTE

**** shell
     当SHELL属性为，命令会在shell环境中执行。即可使用shell本身的特性，
     如管道、文件名通配符、环境变量扩张、家目录"~"扩张等特性。不过，
     Python已提供类似shell的特性，如glob、fnmath、os.path.expanduser()
     等。

     #+BEGIN_QUOTE
     *注意* ：参考后文“安全”小节有关shell=True时的说明。
     #+END_QUOTE

     下面为使用shell=True后使用家目录扩张的例子：

     #+BEGIN_EXAMPLE ipython
       In [60]: subprocess.run(['ls', '~'])
       ls: cannot access '~': No such file or directory
       Out[60]: CompletedProcess(args=['ls', '~'], returncode=2)

       In [61]: subprocess.run(['ls', '~'], shell=True)
       Backup    Documents  Music           Pictures   Templates  Videos
       bin        Downloads  myarchive.tar.gz  Public  test
       Desktop  dwhelper   PDF                               python    test.txt
       Out[61]: CompletedProcess(args=['ls', '~'], returncode=0)
     #+END_EXAMPLE

*** POPEN构造器
    本模块的底层使用Popen类实现，可利用此类更灵活定制run()等函数不能处
    理的情况。

**** class subprocess. *Popen* (args,bufsize=-1,executable=None,stdin=None,stdout=None,stderr=None,preexec_fn=None,close_fds=True,shell=False,cwd=None,env=None,universal_newline=False,startupinfo=None,creationflags=0,restore_signals=True,start_new_session=False,pass_fds=())
     在程序中执行新进程。在POSIX中，使用与os.execvp()类似方法执行子程
     序；Windows中，调用系统CreateProcess()函数。参数使用方法如下：

     - args：为序列或字符串。默认情况下，如果args为序列，则第一个元素
       为需执行的命令；如果为字符串，则依赖不同系统类型，可参考后面关
       于shell和executable参数的介绍。 *首选使用序列，而非字符串。*

       在POSIX中，如果args为字符串，会将字符串解释为可支持程序的路径，
       如"/bin/ls"。但不能更任何参数。

       #+BEGIN_SRC python :session
         import shlex
         import subprocess
         commad_line = '''/usr/bin/viking -input eggs.txt -output "spam spam.txt" -cmd "echo '$MONEY'"'''
         args = shlex.split(commad_line)
         print(args)
         p = subprocess.Popen(args)
       #+END_SRC

       #+RESULTS:
       #+BEGIN_SRC org
       - >>> >>> >>> ['/usr/bin/viking', '-input', 'eggs.txt', '-output', 'spam spam.txt', '-cmd', "echo '$MONEY'"]
       #+END_SRC

       在Windows中，如果args为序列，会转换为字符串，因为系统调用时使用
       的CreateProcess()函数接受字符串参数。

     - shell：默认为False，指定是否使用shell执行程序。如果为True，建议
       使用字符串对象作为args参数值。

       *不建议使用shell=True*

       在POSIX中，当shell=True时，默认使用/bin/sh。ARGS为字符串，将直
       接执行字符串指定的命令，即字符换需与在命令行中直接输入的一样，
       比如引号和转义文件名中的空格的斜杠；如果ARGS为序列，则第一个元
       素表示命令，其余元素表示该命令的参数，即等价于：

       #+BEGIN_EXAMPLE python
         Popen(['/bin/sh', '-c', args[0], args[1], ...])
       #+END_EXAMPLE

       Windows中：（待续）

     - bufsize：与open()函数的bufsize含义相同，用于创建
       stdin/stdout/stderr管道对象：

       - 0：表示不使用缓存。
       - 1：行缓存（仅在universal_newlines=True，如文本模式中有用）。
       - 任何其他正数：使用与该数值接近的缓存大小。
       - 负数（默认）：使用系统默认缓存大小：io.DEFAULT_BUFFER_SIZE。

     - executable：很少使用。当shell=False时，表示替换ARGS指定的命令。

     - stdin、stdout和stderr：见run()函数。

     - preexec_fn：可调用对象，在至今才执行前执行。仅在POSIX中有效。不
       安全。

     - close_fds：默认为True，表示当子进程执行完毕后，文件描述符0、1和
       2将关闭（只在POSIX中有效）。

     - pass_fds：

     - cwd：

     - restore_signals

     - start_new_session

     - env：指定执行子进程时环境变量的映射。默认为None，即使用当前进程
       的环境变量。

     - universal_newlines：当为True时，stdin、stdout和stderr都以文本模
       式打开。默认为False，即以二进制模式打开。

     - startupinfo

     Popen对象支持with上下文管理器，当exit时，标准文件描述符被关闭，进
     程被等待：

     #+BEGIN_EXAMPLE python
       with Popen(['ifconfig'], stdout=PIPE) as proc:
            log.write(proc.stdout.read())
     #+END_EXAMPLE

*** Exceptions
    当新进程开始执行后，抛出的所有错误会被父进程重新抛出。此外，抛出的
    错误有child_traceback属性，为子进程traceback信息的字符串（？？？怎
    么查看）。

    最常见的错误为OSError，比如尝试执行不存在的命令。如果要处理抛出错
    误，需将其包含在内：

    #+BEGIN_EXAMPLE python
      try:
          subprocess.run(['ls', 'qwd'], stderr=subprocess.PIPE, check=True)
      except subprocess.CalledProcessError as proc_why:
          pass
      except OSError as os_why:
          print(os_why)
    #+END_EXAMPLE

    当在Popen中使用无效参数时，会抛出ValueError错误。

    当调用check_all()和check_output()的返回码不为0时，会抛出
    CalledProcessError。

    本模块中所有函数和方法都接受timeout参数，如call()和
    Popen.communicate()，当超时时，会抛出TimeExpired错误。

    本模块中所有错误都继承于SubprocessError。

** 安全
   使用本模块中的函数时，所有字符，包括shell元字符都可安全传入子进程。
   但当shell=True时，需保证为空格和元字符添加引号，避免 [[https://en.wikipedia.org/wiki/Code_injection#Shell_injection][shell注入]] 。

   避免使用shell=True，当使用时，使用shlex.quote()转义命令，转义空格和
   shell元字符。

** Popen对象
   Popen实例有以下方法和属性：

*** Pope. *poll* ()
    轮询检查子进程是否终止。设置和返回returncode属性。？？？

    #+BEGIN_SRC python :session
      from subprocess import Popen
      with Popen(['ls']) as proc:
          proc.communicate()
          print(proc.poll())
          proc.kill()
    #+END_SRC

*** Pope. *wait* (timeout=None)
    等待子进程终止。设置和返回returncode属性。

    如果进程在TIMEOUT参数指定的秒数内没有终止，抛出TimeoutExpired错误。
    可安全捕获此错误，然后在使用wait()方法。

    #+BEGIN_QUOTE
    *注意* ：当使用stdout=PIPE或stderr=PIPE，且子进程想管道输出内容过
    多，导致阻塞操作心态管道接受更多数据时，会造成“锁死”。使用
    Popen.communicate可避免此情况发生。？？？
    #+END_QUOTE

    #+BEGIN_QUOTE
    *注意* ：此函数使用非阻塞调用和短休眠。如需使用同步等待，可参考
    asyncio.create_subprocess_exec。
    #+END_QUOTE

*** Pope. *communicate* (input=None,timeout=None)
    与子进程交流：向stdin发送数据。从stdout和stderr读取数据，直到遇到
    EOF。等待子进程结束。

    返回值为(stdout_data, stderr_data)组成的元组，都为bytes对象。如果
    universal_newlines参数为True，则为字符串。

    input：可选参数，为向子进程发送的数据，如果为None则不发送数据。须
    为bytes对象，如果universal_newlines为True，则为字符串。

    如需指定input参数，向stdin发送数据，则创建Popen实例时，需声明
    stdin=PIPE；同样，如需从返回元组中获取数据，则需声明stdout=PIPE和
    stderr=PIPE，否则只能获取None。

    timeout：如果子进程在TIMEOUT参数指定的时间内为终止，则抛出
    TimeoutExpired错误。捕获此错误，再重新使用communicate()方法不会丢
    失任何输出数据。

    如果抛出TimeoutExpired错误后子进程还没被干掉，需手动干掉，再接受
    communicate()操作：

    #+BEGIN_EXAMPLE python
      proc = subprocess.Popen(...)
      try:
          outs, errs = proc.communicate(timeout=15)
      except TimeoutExpired:
             proc.kill()
             outs, errs = proc.communicate()
    #+END_EXAMPLE

    #+BEGIN_QUOTE
    *注意* ：此方法读取的数据会存储在内存中，所以当数据量大或无限时，
    不要使用此方法。
    #+END_QUOTE

*** Pope. *send_signal* (signal)
    向子进程发生SIGNAL信号。
*** Pope. *terminate* ()
    停止子进程。在POSIX中为发送SIGTERM，在Windows中，则是调用Win32 API
    的TerminateProcess()函数。

*** Pope. *kill* ()
    干掉子进程。在POSIX中为发送SIGKILL信号，在Windows中等价于terminate()。
*** Pope. *args*
    传入Popen的ARGS参数，为序列或字符串。
*** Pope. *stdin*
    如果stdin参数为PIPE，则此属性为可写流对象（与open()函数返回值相同）。
    如果universal_newlines参数为True，则为文本流，否则为二进制流。

    如果stdin参数为None，则此属性值也为None。

    #+BEGIN_SRC python :session
      from subprocess import *
      with Popen(['cat'], stdout=PIPE, stdin=PIPE) as proc:
          proc.stdin.write(b'123')
          print(proc.communicate())
    #+END_SRC

    #+RESULTS:
    #+BEGIN_SRC org
    - ... ... ... 3
    - (b'123', None)
    #+END_SRC

*** Pope. *stdout*
    如果stdout参数为PIPE，则此属性为可读流对象（与open()函数返回值相
    同）。读取此流对象的返回值为子进程的输出值。如果universal_newlines
    参数为True，则为你文本流，否则为二进制流。

    如果stdout参数为None，则此属性值也为None。

*** Pope. *stderr*
    如果stderr参数为PIPE，则此属性为可读流对象（与open()函数返回值相
    同）。读取此流对象的返回值为子进程的输出值。如果universal_newlines
    参数为True，则为你文本流，否则为二进制流。

    如果stderr参数为None，则此属性值也为None。

    #+BEGIN_QUOTE
    *警告* ：避免OS的管道被填满，造成阻塞子进程，进而“锁死”，应使用
    communicate()，而非.stdin.write、stdout.read或stderr.read。
    #+END_QUOTE

*** Pope. *pid*
    返回子进程ID，如果shell=True，则是shell进程的ID。

*** Pope. *returncode*
    子进程返回码，由poll()和wait()设置（间接由communicate()设置）。

    None表示进程还没终止。

    负数-N，表示进程被信号N终止，（仅POSIX有效）。

** Windows Popen帮助 （待续）
*** 常量
** 旧的高阶API
   Python3.5前，下面3个函数旧API函数。现在可使用run()替换，但很多现有
   的代码都使用他们。

*** subprocess. *call* (args,*,stdin=None,stdout=None,stderr=None,shell=False,timeout=None)
    执行ARGS指定的命令，等待执行结束，返回returncode属性。

    除不支持input和check参数外，等价于：

    #+BEGIN_EXAMPLE python
      run(...).returncode
    #+END_EXAMPLE

    除TIMEOUT参数外，此函数的所有参数都传递给Popen接口。

    #+BEGIN_QUOTE
    *注意* ： 不要使用stdout=PIPE或stderr=PIPE。当OS管道没有被读取，且
    此函数产生的数据填满管道后，会造成阻塞。
    #+END_QUOTE

*** subprocess. *check_all* (args,*,stdin=None,stdout=None,stderr=None,shell=False,timeout=None)
    执行ARGS指定的命令，等待命令完成。如果返回码为0，返回，否则抛出
    CalledProcessError错误。可从CalledProcessError中获取returncode属性。

    除不支持input参数外，等价于：

    #+BEGIN_EXAMPLE python
      run(..., check=True)
    #+END_EXAMPLE

    除TIMEOUT参数外，此函数的所有参数都传递给Popen接口。

    #+BEGIN_QUOTE
    *注意* ： 不要使用stdout=PIPE或stderr=PIPE。当OS管道没有被读取，且
    此函数产生的数据填满管道后，会造成阻塞。
    #+END_QUOTE

*** subprocess. *check_output* (args,*,stdin=None,stdout=None,stderr=None,shell=False,timeout=None)
    执行命令，返回执行输出。

    如果返回码不为0，则抛出CalledProcessError。可通过
    CalledProcessError的returncode和output属性分别获取返回码和输出内容。

    除不支持input参数外，大多数参数都传递给run()函数的接口。等价于：

    #+BEGIN_EXAMPLE python
      run(..., check=True, stdout=PIPE).stdout
    #+END_EXAMPLE

    返回结果为bytes对象，编码形式依赖于命令执行环境。所以应该在应用层
    级上解码。？？？

    设置universal_newlines=True后，可使返回字符串。

    如需捕获stderr，可声明参数stderr=subprocess.STDOUT：

    #+BEGIN_SRC python :session
      import subprocess
      subprocess.check_output(
          'ls none_exists_file; exit 0',
          stderr=subprocess.STDOUT,
          shell=True)
    #+END_SRC

    #+RESULTS:
    #+BEGIN_SRC org
    - ... ... ... b"ls: cannot access 'none_exists_file': No such file or directory\n"
    #+END_SRC

** 使用subprocess模块替换旧函数

   #+BEGIN_QUOTE
   *注意* ： 本节中所有被替换的函数如果不能找到执行命令，都不会抛出错
   误；而替换函数会抛出OSError错误。

   此外，使用check_output()替换的版本中，如果执行返回码不为0，会抛出
   CalledProcessError错误，可使用output属性获取具体内容。
   #+END_QUOTE

*** 替换/bin/sh的反引号

    #+BEGIN_EXAMPLE sh
      output=`mycmd myarg`
    #+END_EXAMPLE

    可替换为：

    #+BEGIN_EXAMPLE python
      output = check_output(['mycmd', 'myarg'])
    #+END_EXAMPLE

*** 替换管道

    #+BEGIN_EXAMPLE sh
      oputput=`dmesg | grep hda`
    #+END_EXAMPLE

    可替换为：

    #+BEGIN_EXAMPLE python
      p1 = Popen(['dmesg'], stdout=PIPE)
      p2 = Popen(['grep', 'hda'], stdin=p1.stdout, stdout=PIPE)
      p1.stdout.close() # 保证p1在p2退出时接受SIGPIPE信号
      output=p2.communicate()[0]
    #+END_EXAMPLE

    如果输如可信任，可直接使用shell的管道：

    #+BEGIN_EXAMPLE python
      output = check_output('dmesg | grep hda', shell=True)
    #+END_EXAMPLE

*** 替换os.system()函数

    #+BEGIN_EXAMPLE python
      sts = os.system('mycmd' + ' myarg')
      # 替换为
      sts = call('mycmd' + ' myarg', shell=True)
    #+END_EXAMPLE

    更实际的例子为：

    #+BEGIN_EXAMPLE python
      try:
          retcode = call('mycmd', + ' myarg', shell=True)
          if rtcode < 0:
             print('子进程被信号中断', -rtcode, file=sys.stderr)
          else:
             print('子进程返回', rtcode, file=sys.stderr)
      except OSError as e:
          print('执行失败：', e, file=sys.stderr)
    #+END_EXAMPLE

*** 替换os.spawn类函数 （待续）
*** 替换os.popen()、os.popen2()和os.popen3() （待续）
*** 替换popen2模块的函数 （待续）
** 遗留Shell激活函数 （存在于2.x中，可能不安全，待续）
** 注意点 （待续）
*** 在Windows中将参数序列转换为字符串
* sched - 事件调度
** scheduler对象
* queue - 同步队列类
  queue模块实现了多生产者和多消费者队列。当多个线程需安全交换信息时尤
  其有用。Queue类实现了所有必须的互斥锁语义。

  本模块实现类3种对象，区别在于获取元素的顺序。在FIFO队列中，先添加的
  元素先被获取；LIFO队列中，后添加的元素被先获取；在优先队列中，所有元
  素一致处于排序状态，值最小额元素被先获取（底层利用了heapq模块）。

** 类和Exceptions
*** class queue. *Queue* (maxsize=0)
    构造FIFO队列。

    maxsize：整数。表示队列中最多可有多少个元素。当达到上限时，如果尝
    试向其中插入元素，则阻塞至其中有元素被获取为止。如果值<=0，则表示
    元素可有无限多个。

*** class queue. *LifoQueue* (maxsize=0)
    构造LIFO队列。

    maxsize：同Queue。

*** class queue. *PriorityQueue* (maxsize=0)
    构造优先队列。

    maxsize：同Queue。

    最先获取“最小值”，sorted(list(entries))[0]。其中元素典型的模式为元
    组：(priority_number, data)。

*** exception queue. *Empty*
    当使用block=False调用get()方法（或get_nowait()），队列对象中没有元
    素时抛出。

*** exception queue. *Full*
    当使用block=False调用put()方法（或put_nowait()），队列对象中元素已
    到达上限时抛出。

** Queue对象
*** 普通方法
**** Queue. *qsize* ()
     返回队列大小近似值。如果qsize()>0，并不能保证再调用get()不会阻塞，
     如果qsize()<maxsize，并不能保证再调用put()不会阻塞。

**** Queue. *empty* ()
     如果队列为空则返回True。如果返回True，并不能保证再调用put()不会阻
     塞，如果为False，并不能保证再调用get()不会阻塞。

**** Queue. *full* ()
     如果队列中元素个数达到上限则返回True。如果返回True，并不能保证再
     调用get()不会阻塞，如果返回False，并不能保证再调用put()不会阻塞。

**** Queue. *put* (item,block=True,timeout=None)
     向队列中添加元素。

     如果可选参数block=True且timeout=None（默认），且队列中元素个数达
     到上限，则阻塞至队列中有剩余空间。

     如果timeout参数为正数，则阻塞最多该参数指定时长，如果超时后队列中
     没有剩余空间，则抛出Full错误。

     当block=False时（忽视timeout参数），如果队列中有剩余空间则立即添加，否则抛出Full错误。

**** Queue. *put_nowait* (item)
     等价于put(item, block=False)
**** Queue. *get* (block=True,timeout=None)
     从队列中移除并返回一个元素。

     如果block=True且timeout=None（默认），且队列中没有元素，则阻塞至
     队列中有元素为止。

     如果timeout参数为正数，则最多阻塞该参数指定时长。如果超时后队列中
     依然没有元素，则抛出Empty错误。

     当block=False时（忽视timeout参数），如果队列中有元素则立即返回，
     否则抛出Empty错误。

**** Queue. *get_nowait* ()
     等价于get(block=False)
*** 跟踪方法
    下面两个方法可用来跟踪队列等待任务是否被后台消费线程完全处理。

**** Queue. *task_done* ()
     用于提示单个队列任务完成。被队列消费线程使用。当使用get()获取并处
     理完成一个元素后，再调用task_done()表示对当前元素的处理已完成。

     如果当前join()正阻塞，每次调用后都会检查是否调用次数达到队列中元
     素个数，如果达到则解除锁定。

     如果调用次数超过向队列中添加的元素个数，则抛出ValueError错误。

**** Queue. *join* ()
     调用后，阻塞直到队列中所有元素都被消费线程获取并处理。

     每当向队列中添加一个元素时，未完成任务数量增加1；当消费线程调用
     task_done()后，表示从队列中获取元素并完成处理，未完成任务数量减1。
     当未完成任务数量减小至0时，join()解除锁定。

     #+BEGIN_EXAMPLE python
       import queue
       import threading

       q = queue.Queue()

       do_work = print
       threads = []
       worker_threads_num = 4

       def worker():
           while True:
               item = q.get()
               if item is None:
                   break
               do_work(item)
               q.task_done()

       for i in range(worker_threads_num):
           t = threading.Thread(target=worker)
           t.start()
           threads.append(t)

       for item in range(10):
           q.put(item)

       # 阻塞直到所有任务完成
       q.join()
       print('所有任务完成')

       # 停止workers

       for i in range(worker_threads_num):
           q.put(None)

       for t in threads:
           t.join()
           pass

       print('主线程结束')
     #+END_EXAMPLE

** 其他
   #+BEGIN_QUOTE
   *另见* ：

   - class multiprocessing.Queue：用于多进程的队列类。

   - collections.deque：使用append()和popleft()方法可实现，无需“锁”的队列。

   #+END_QUOTE

* 支持上面模块服务的底层模块
** dummy_threading
** _thread
** _dummy_thread
